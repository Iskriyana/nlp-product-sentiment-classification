{
 "cells": [
  {
   "attachments": {
    "06495149-b137-49ba-a11f-77ad98aa7624.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAFoCAYAAAB65WHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\nZf4/NF3KMIOsg28AAAAASUVORK5CYII=\n"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text_star.png](attachment:06495149-b137-49ba-a11f-77ad98aa7624.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals of this notebook are: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* to identify the best models in terms of F1 score on the training and validation data for the **text** part of the data\n",
    "* F1 was chosen, as the dataset at hand is imbalanced. \n",
    "* The following models will be evaluated: \n",
    "    * bag-of-words\n",
    "    * a fully connected NN with \"homegrown\" embeddings layer\n",
    "    * a fully connected NN with pre-trained embeddings layer\n",
    "    * a \"homegrown\" embeddings layer with LSTM \n",
    "    * a pre-trained embeddings layer with LSTM\n",
    "    * a \"homegrown\" embeddings layer with Conv1D \n",
    "    * a pre-trained embeddings layer with Conv1D\n",
    "* to regularise the top 2 models in order for them to generalise better on unseen data\n",
    "* to define the final model for the NLP part (see at the end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "import json\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "from keras import utils\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folders\n",
    "home = os.getenv(\"HOME\")\n",
    "nlp_repo = os.path.join(home, 'git/nlp-product-sentiment-classification')\n",
    "\n",
    "# data\n",
    "train_csv_path = os.path.join(nlp_repo, 'data/03_processed/Train.csv')\n",
    "train_descr = pd.read_csv(train_csv_path)\n",
    "\n",
    "test_csv_path = os.path.join(nlp_repo, 'data/03_processed/Test.csv')\n",
    "test_descr = pd.read_csv(test_csv_path)\n",
    "\n",
    "# encoded tokens\n",
    "preprocessed_corpus_path_TF = os.path.join(\n",
    "    nlp_repo, 'data/03_processed/product_descr_preprocessed_TF.p')\n",
    "\n",
    "preprocessed_corpus_path_TF_oh = os.path.join(\n",
    "    nlp_repo, 'data/03_processed/product_descr_preprocessed_TF_oh.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Parameter Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log results to logs/experiments/all_models_regularisation_2020_12_19-20:39\n"
     ]
    }
   ],
   "source": [
    "logging = True\n",
    "\n",
    "saving = True\n",
    "\n",
    "PARAMS = {\n",
    "\n",
    "    # Define experiment name:\n",
    "    'experiment_name': 'all_models_regularisation',\n",
    "\n",
    "    # List of models\n",
    "    'model_type': ['bow', 'fc_emb', 'fc_transf', 'lstm', 'lstm_transf', 'conv1d', 'conv1d_transf'],\n",
    "\n",
    "    # Parameters general:\n",
    "    'number_of_classes': len(np.unique(train_descr['Sentiment'])),\n",
    "    'n_splits': 4,\n",
    "    'seed': 42,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 15,\n",
    "    'hidden_units': 32,\n",
    "\n",
    "    # NLP Parameters\n",
    "    # max_words = vocabulary size = our samples - number of most frequent words.\n",
    "    # they are set to 10.000, although in this case there are less.\n",
    "    # this is done to parametise the code.\n",
    "    # aleternatively, it can be set to the length of our vocabulary = word_index\n",
    "    'max_words': 10000,\n",
    "\n",
    "    # embedding_size = embedding dimensionality\n",
    "    'embedding_size': 30,\n",
    "\n",
    "    # parameters for pre-trained word embeddings:\n",
    "    'embedding_dim': 100,\n",
    "\n",
    "    # parameters for the Conv1D:\n",
    "    'conv_window': 5,\n",
    "\n",
    "    # cross-fold validation:\n",
    "    'k': 4,\n",
    "\n",
    "    # regularisation types\n",
    "    'reg_mode': ['l2', 'dropout', 'combi']\n",
    "}\n",
    "\n",
    "logdir = f'logs/experiments/{PARAMS[\"experiment_name\"]}_' + \\\n",
    "    datetime.datetime.now().strftime(\"%Y_%m_%d-%H:%M\")\n",
    "logdir_tb = f'logs/tensorboard/experiments/{PARAMS[\"experiment_name\"]}_' + \\\n",
    "    datetime.datetime.now().strftime(\"%Y_%m_%d-%H:%M\")\n",
    "\n",
    "# create logging folder and tensorboard callback function\n",
    "if logging:\n",
    "    print(f'Log results to {logdir}')\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    tensorboard_callbacks = [tf.keras.callbacks.TensorBoard(log_dir=logdir_tb)]\n",
    "\n",
    "else:\n",
    "    logdir = ''\n",
    "    logdir_tb = ''\n",
    "    tensorboard_callbacks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Tokens and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path_to_corpus, one_hot=False):\n",
    "    \"\"\"\n",
    "    Loads the tokenised text and produces the labels. \n",
    "    If one-hot encoded is not needed, the data is also padded. \n",
    "\n",
    "    Args: \n",
    "        - path_to_corpus - path to tokenised text\n",
    "        - one_hot - if the model is bag-of-words, one-hot-encoded data is used. The default is False\n",
    "\n",
    "    Returns:\n",
    "        - sequences - tokenised text\n",
    "        - word_index - a dictionary having as key the word and as value its index\n",
    "        - data - in case of one_hot=False the padded sequences\n",
    "        - labels - the sentiments \n",
    "    \"\"\"\n",
    "\n",
    "    sequences, word_index = pd.read_pickle(path_to_corpus)\n",
    "\n",
    "    if not one_hot:\n",
    "        # max_len = sequence length - the text is cut off after this number of words\n",
    "        # in this case it is defined as the maximum sequence length in our list of tokenised sequences\n",
    "        max_len = np.max([len(x) for x in sequences])\n",
    "        data = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            sequences, maxlen=max_len)\n",
    "    else:\n",
    "        data = sequences\n",
    "\n",
    "    labels = train_descr.loc[:, 'Sentiment'].to_list()\n",
    "\n",
    "    return sequences, word_index, data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For bag-of-words with one-hot encoded tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6364, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_oh, word_index_oh, data_oh, labels_oh = load_data(\n",
    "    preprocessed_corpus_path_TF_oh, one_hot=True)\n",
    "sequences_oh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the models with embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sequences after padding is (6364, 25)\n"
     ]
    }
   ],
   "source": [
    "sequences, word_index, data, labels = load_data(preprocessed_corpus_path_TF)\n",
    "print(f'Shape of sequences after padding is {data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add max_len to parameters dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = np.max([len(x) for x in sequences])\n",
    "PARAMS['max_len'] = max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for the Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = './glove.6B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((PARAMS['max_words']+1, PARAMS['embedding_dim']))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < PARAMS['max_words']:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, labels):\n",
    "    \"\"\"\n",
    "    Splits the data into train and test data sets.\n",
    "    One-hot encodes the labels.\n",
    "\n",
    "    Args: \n",
    "        - data - preprocessed text_data\n",
    "        - labels - sentiments\n",
    "\n",
    "    Returns: \n",
    "        - x_train, y_train_oh - training text data & the corresponding one-hot encoded labels\n",
    "        - x_test, y_test_oh - test text data & the corresponding one-hot encoded labels\n",
    "    \"\"\"\n",
    "    dimensions_labels = PARAMS['number_of_classes']\n",
    "\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    data = data[indices]\n",
    "    labels = np.asarray(labels)\n",
    "    labels = labels[indices]\n",
    "\n",
    "    training_samples = int(0.8 * len(data))\n",
    "\n",
    "    x_train = data[:training_samples]\n",
    "    y_train = labels[:training_samples]\n",
    "\n",
    "    x_test = data[training_samples:]\n",
    "    y_test = labels[training_samples:]\n",
    "\n",
    "    y_train_oh = tf.one_hot(indices=y_train, depth=dimensions_labels)\n",
    "    y_train_oh = np.asarray(y_train_oh)\n",
    "\n",
    "    y_test_oh = tf.one_hot(indices=y_test, depth=dimensions_labels)\n",
    "    y_test_oh = np.asarray(y_test_oh)\n",
    "\n",
    "    return x_train, y_train_oh, x_test, y_test_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = train_test_split(data, labels)\n",
    "\n",
    "x_train_oh, y_train_oh, x_test_oh, y_test_oh = train_test_split(\n",
    "    data_oh, labels_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for Model Generation & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Oversampling the Minority Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE = Synthetic Minority Oversampling Technique. It was chosen, because the data set is small and oversampling will, even if synthetically, increase it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_smote(x_train, y_train):\n",
    "    \"\"\"\n",
    "    This function produces an oversampled set of train data using SMOTE\n",
    "\n",
    "    Args: \n",
    "        - x_train - imbalanced train features (= text descriptions)\n",
    "        - y_train - imbalanced train labels (= sentiments)\n",
    "\n",
    "    Returns: \n",
    "        - x_train - oversampled train features (= text descriptions)\n",
    "        - y_train - oversampled train labels (= sentiments)\n",
    "\n",
    "    \"\"\"\n",
    "    oversample = SMOTE()\n",
    "    x_train, y_train = oversample.fit_resample(x_train, y_train)\n",
    "\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Generate the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(model_type, params_dict):\n",
    "    \"\"\"\n",
    "    This function compiles a model according to the chosen model_type\n",
    "    Args: \n",
    "        - model_type - which model type is to be used\n",
    "        - params_dict - dictionary with parameters for the model\n",
    "\n",
    "    Returns: \n",
    "        - a compiled model\n",
    "    \"\"\"\n",
    "\n",
    "    # set parameters\n",
    "    hidden_units = params_dict['hidden_units']\n",
    "    dimensions_labels = params_dict['number_of_classes']\n",
    "    max_words = params_dict['max_words']\n",
    "    max_len = params_dict['max_len']\n",
    "    embedding_size = params_dict['embedding_size']\n",
    "    embedding_dim = params_dict['embedding_dim']\n",
    "    conv_window = params_dict['conv_window']\n",
    "\n",
    "    # fully connected bag-of-words\n",
    "    if model_type == params_dict['model_type'][0]:\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(hidden_units, activation='relu',\n",
    "                               input_shape=(max_words, )))\n",
    "        model.add(layers.Dense(hidden_units, activation='relu'))\n",
    "        model.add(layers.Dense(dimensions_labels, activation='softmax'))\n",
    "\n",
    "    # fully connected & \"homegrown\" embeddings layer\n",
    "    elif model_type == params_dict['model_type'][1]:\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Embedding(max_words+1,\n",
    "                                   embedding_size, input_length=max_len))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(hidden_units, activation='relu'))\n",
    "        model.add(layers.Dense(hidden_units, activation='relu'))\n",
    "        model.add(layers.Dense(dimensions_labels, activation='softmax'))\n",
    "\n",
    "    # fully connected & a pre-trained embeddings layer\n",
    "    elif model_type == params_dict['model_type'][2]:\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Embedding(max_words+1,\n",
    "                                   embedding_dim, input_length=max_len))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(hidden_units, activation='relu'))\n",
    "        model.add(layers.Dense(hidden_units, activation='relu'))\n",
    "        model.add(layers.Dense(dimensions_labels, activation='softmax'))\n",
    "\n",
    "        model.layers[0].set_weights([embedding_matrix])  # !!!! important !!!!\n",
    "        model.layers[0].trainable = False\n",
    "\n",
    "    # lstm with \"homegrowm\" embeddings layer\n",
    "    elif model_type == params_dict['model_type'][3]:\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Embedding(max_words+1,\n",
    "                                   embedding_dim, input_length=max_len))\n",
    "        model.add(layers.LSTM(hidden_units))\n",
    "        model.add(layers.Dense(hidden_units, activation='relu'))\n",
    "        model.add(layers.Dense(hidden_units, activation='relu'))\n",
    "        model.add(layers.Dense(dimensions_labels, activation='softmax'))\n",
    "\n",
    "    # lstm with pre-trained embeddings layer\n",
    "    elif model_type == params_dict['model_type'][4]:\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Embedding(max_words+1,\n",
    "                                   embedding_dim, input_length=max_len))\n",
    "        model.add(layers.LSTM(hidden_units))\n",
    "        model.add(layers.Dense(hidden_units, activation='relu'))\n",
    "        model.add(layers.Dense(hidden_units, activation='relu'))\n",
    "        model.add(layers.Dense(dimensions_labels, activation='softmax'))\n",
    "\n",
    "        model.layers[0].set_weights(\n",
    "            [embedding_matrix])  # !!!! important !!!!\n",
    "        model.layers[0].trainable = False\n",
    "\n",
    "    # conv1D with \"homegrown\" embeddings layer\n",
    "    elif model_type == params_dict['model_type'][5]:\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Embedding(max_words+1,\n",
    "                                   embedding_size, input_length=max_len))\n",
    "\n",
    "        model.add(layers.Conv1D(\n",
    "            hidden_units,  # features to be extracted\n",
    "            conv_window,  # convolutional window size\n",
    "            activation='relu',\n",
    "        ))\n",
    "        model.add(layers.Conv1D(hidden_units, conv_window, activation='relu'))\n",
    "        model.add(layers.GlobalMaxPooling1D())\n",
    "        model.add(layers.Dense(dimensions_labels, activation='softmax'))\n",
    "\n",
    "    # conv1D with pre-trained embeddings layer\n",
    "    elif model_type == params_dict['model_type'][6]:\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Embedding(max_words+1,\n",
    "                                   embedding_dim, input_length=max_len))\n",
    "\n",
    "        model.add(layers.Conv1D(hidden_units, conv_window, activation='relu',))\n",
    "        model.add(layers.Conv1D(hidden_units, conv_window, activation='relu'))\n",
    "        model.add(layers.GlobalMaxPooling1D())\n",
    "        model.add(layers.Dense(dimensions_labels, activation='softmax'))\n",
    "\n",
    "        model.layers[0].set_weights([embedding_matrix])  # !!!! important !!!!\n",
    "        model.layers[0].trainable = False\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Training with Cross-Validation (incl. Upsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Due to the small data quantity, cross-validation will be used to train the models.\n",
    "* Furthermore, as the classes are imbalanced, an oversampling will be introduced. ! It is important to generate the training and validation sets BEFORE the upsampling. Otherwise the exact same observations could be present in both the train and validation sets, i.e. data leakage would occur. This can allow the model to simply memorize specific data points and cause overfitting and poor generalization on unseen data. \n",
    "* The oversampling will be applied only on the train set in order to ensure that the model generalises well on unseen data (validation set and ultimately test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_cross_val(k, model, model_type, train_data, train_labels, epochs, batch_size, reg_mode=None, reg=False):\n",
    "    \"\"\"\n",
    "    This function applies a k-fold cross-validation and saves the model history per fold, epoch and if \n",
    "    given per regularisation type.\n",
    "\n",
    "    Args: \n",
    "        - k - number of folds\n",
    "        - model - compiled model\n",
    "        - model_type - which model type is to be used\n",
    "        - train_data - the tokenised, padded and shuffled sequences\n",
    "        - train_labels - labels \n",
    "        - epochs - number of epochs\n",
    "        - batch_size - batch size\n",
    "        - reg_mode - which type of regularisaion is used, if any\n",
    "        - reg - is regularisation applied\n",
    "    \"\"\"\n",
    "    num_val_samples = len(train_data) // k\n",
    "\n",
    "    for i in range(k):\n",
    "        print('processing fold #', i)\n",
    "        val_data = train_data[i * num_val_samples:(i + 1) * num_val_samples]\n",
    "        val_labels = train_labels[i *\n",
    "                                  num_val_samples:(i + 1) * num_val_samples]\n",
    "\n",
    "        partial_train_data = np.concatenate(\n",
    "            [train_data[:i * num_val_samples],\n",
    "             train_data[(i + 1) * num_val_samples:]],\n",
    "            axis=0)\n",
    "\n",
    "        partial_train_labels = np.concatenate(\n",
    "            [train_labels[:i * num_val_samples],\n",
    "             train_labels[(i + 1) * num_val_samples:]],\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        partial_train_data_ovs, partial_train_labels_ovs = oversample_smote(\n",
    "            partial_train_data, partial_train_labels)\n",
    "\n",
    "        history = model.fit(\n",
    "            partial_train_data_ovs, partial_train_labels_ovs,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            validation_data=(val_data, val_labels),\n",
    "            # callbacks=tensorboard_callbacks\n",
    "        )\n",
    "\n",
    "        if not reg:\n",
    "            np.save(f'history_{model_type}_fold_{i}.npy', history.history)\n",
    "\n",
    "        else:\n",
    "            np.save(\n",
    "                f'history_{model_type}_{reg_mode}_fold_{i}.npy', history.history)\n",
    "\n",
    "    if saving:\n",
    "        if not reg:\n",
    "            path = os.path.join(logdir, f'{model_type}')\n",
    "            model.save(path)\n",
    "        else:\n",
    "            path = os.path.join(logdir, f'{model_type}_{reg_mode}')\n",
    "            model.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Baseline Model for Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea Model: https://machinelearningmastery.com/imbalanced-multiclass-classification-with-the-glass-identification-dataset/\n",
    "\n",
    "Metrics: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "\n",
    "* As the Fully Connected NN model's the precision, recall and f1 are calculcated unweighted, precision, recall & f1 are chosen to be macro\n",
    "* According to the sklearn-documentation this method calculates the metrics for each label, and finds their unweighted mean and does not take label imbalance into account.\n",
    "* Example documentation for precision: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n",
    "\n",
    "Note: test_[metric] is validation and therefore comparable to the val_[metric] of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = DummyClassifier(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=PARAMS['k'], random_state=PARAMS['seed'])\n",
    "\n",
    "scores = cross_validate(\n",
    "    model_base, x_train, y_train, scoring=scoring, cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean fit_time: 0.001 (0.000)\n",
      "Mean score_time: 0.015 (0.001)\n",
      "Mean test_accuracy: 0.592 (0.020)\n",
      "Mean test_precision_macro: 0.148 (0.005)\n",
      "Mean test_recall_macro: 0.250 (0.000)\n",
      "Mean test_f1_macro: 0.186 (0.004)\n"
     ]
    }
   ],
   "source": [
    "for metric, score in scores.items():\n",
    "    print(f'Mean {metric}: %.3f (%.3f)' %\n",
    "          (np.mean(score), np.std(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model per Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 321,220\n",
      "Trainable params: 321,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "processing fold # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: logs/experiments/all_models_regularisation_2020_12_19-20:39/bow/assets\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 25, 30)            300030    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                24032     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 325,250\n",
      "Trainable params: 325,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "processing fold # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/experiments/all_models_regularisation_2020_12_19-20:39/fc_emb/assets\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 100)           1000100   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2500)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                80032     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 1,081,320\n",
      "Trainable params: 81,220\n",
      "Non-trainable params: 1,000,100\n",
      "_________________________________________________________________\n",
      "processing fold # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/experiments/all_models_regularisation_2020_12_19-20:39/fc_transf/assets\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 25, 100)           1000100   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 1,019,368\n",
      "Trainable params: 1,019,368\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "processing fold # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/experiments/all_models_regularisation_2020_12_19-20:39/lstm/assets\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 25, 100)           1000100   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 1,019,368\n",
      "Trainable params: 19,268\n",
      "Non-trainable params: 1,000,100\n",
      "_________________________________________________________________\n",
      "processing fold # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iskriyanavasileva/opt/anaconda3/envs/nlp-sent/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "for model_type in PARAMS['model_type']:\n",
    "    model = generate_model(model_type, PARAMS)\n",
    "\n",
    "    if model_type == PARAMS['model_type'][0]:\n",
    "        train_with_cross_val(PARAMS['k'], model, model_type, x_train_oh,\n",
    "                             y_train_oh, PARAMS['epochs'], PARAMS['batch_size'])\n",
    "    else:\n",
    "        train_with_cross_val(\n",
    "            PARAMS['k'], model, model_type, x_train, y_train, PARAMS['epochs'], PARAMS['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Metrics Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_all_df = pd.DataFrame()\n",
    "\n",
    "for model_type in PARAMS['model_type']:\n",
    "    for i in range(PARAMS['k']):\n",
    "        history = np.load(\n",
    "            f'history_{model_type}_fold_{i}.npy', allow_pickle='TRUE').item()\n",
    "        history['model_type'] = model_type\n",
    "\n",
    "        history_df = pd.DataFrame(history)\n",
    "\n",
    "        history_df['f1'] = 2 * history_df.precision * \\\n",
    "            history_df.recall/(history_df.precision + history_df.recall)\n",
    "        history_df['f1'] = history_df['f1'].fillna(0)\n",
    "        history_df['val_f1'] = 2 * history_df.val_precision * \\\n",
    "            history_df.val_recall / \\\n",
    "            (history_df.val_precision + history_df.val_recall)\n",
    "        history_df['val_f1'] = history_df['val_f1'].fillna(0)\n",
    "\n",
    "        history_all_df = history_all_df.append(history_df)\n",
    "\n",
    "history_all_df\n",
    "history_all_df_pivot = history_all_df.pivot_table(columns=['model_type'],\n",
    "                                                  values=['loss', 'val_loss', 'accuracy', 'val_accuracy', 'precision', 'val_precision', 'f1', 'val_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_all_df_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick the 2 models with the highest F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All of the models outperform the baseline model in all the metrics\n",
    "* The models before regularisation with the best F1-score are the bag-of-words and the \"homegrown\" embeddings\n",
    "* These two models will be optimised further and the best performing one (see regularisation section) will be used for the final multi-input model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_all_df_pivot.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_all_df_pivot.loc['val_f1', ].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Summary Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_all_grouped = history_all_df.groupby('model_type').mean()\n",
    "history_all_grouped = history_all_grouped.reset_index()\n",
    "history_all_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Plotting Train vs. Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_vs_validation(models_list, history_df, params, reg=False):\n",
    "    '''\n",
    "    Function to plot a training metric vs. a validation one \n",
    "\n",
    "    Args:\n",
    "        - models_list - models, for which the results will be visualised \n",
    "        - history_df - dataframe with results from model training\n",
    "        - params - parameters dictionary\n",
    "        - reg - if there was regularisation or not\n",
    "    '''\n",
    "\n",
    "    epochs = params['epochs']\n",
    "    epochs_graph = range(1, params['epochs'] + 1)\n",
    "    folds = params['k']\n",
    "\n",
    "    if not reg:\n",
    "        for model_type in models_list:\n",
    "            fig = plt.figure(figsize=(15, 5))\n",
    "            ax = plt.subplot(131)\n",
    "\n",
    "            epochs = params['epochs']\n",
    "            epochs_graph = range(1, params['epochs'] + 1)\n",
    "            folds = params['k']\n",
    "\n",
    "            mask = history_df.loc[:, 'model_type'] == model_type\n",
    "\n",
    "            loss_all = history_df.loc[mask, 'loss'].to_list()\n",
    "            loss_prep = [loss_all[i * epochs:(i + 1) * epochs]\n",
    "                         for i in range(folds)]\n",
    "            loss = [np.mean([x[i] for x in loss_prep]) for i in range(epochs)]\n",
    "\n",
    "            val_loss_all = history_df.loc[mask, 'val_loss'].to_list()\n",
    "            val_loss_prep = [\n",
    "                val_loss_all[i * epochs:(i + 1) * epochs] for i in range(folds)]\n",
    "            val_loss = [np.mean([x[i] for x in val_loss_prep])\n",
    "                        for i in range(epochs)]\n",
    "\n",
    "            accuracy_all = history_df.loc[mask, 'accuracy'].to_list()\n",
    "            accuracy_prep = [\n",
    "                accuracy_all[i * epochs:(i + 1) * epochs] for i in range(folds)]\n",
    "            accuracy = [np.mean([x[i] for x in accuracy_prep])\n",
    "                        for i in range(epochs)]\n",
    "\n",
    "            val_accuracy_all = history_df.loc[mask, 'val_accuracy'].to_list()\n",
    "            val_accuracy_prep = [val_accuracy_all[i *\n",
    "                                                  epochs:(i + 1) * epochs] for i in range(folds)]\n",
    "            val_accuracy = [np.mean([x[i] for x in val_accuracy_prep])\n",
    "                            for i in range(epochs)]\n",
    "\n",
    "            f1_all = history_df.loc[mask, 'f1'].to_list()\n",
    "            f1_prep = [f1_all[i * epochs:(i + 1) * epochs]\n",
    "                       for i in range(folds)]\n",
    "            f1 = [np.mean([x[i] for x in f1_prep]) for i in range(epochs)]\n",
    "\n",
    "            val_f1_all = history_df.loc[mask, 'val_f1'].to_list()\n",
    "            val_f1_prep = [val_f1_all[i * epochs:(i + 1) * epochs]\n",
    "                           for i in range(folds)]\n",
    "            val_f1 = [np.mean([x[i] for x in val_f1_prep])\n",
    "                      for i in range(epochs)]\n",
    "\n",
    "            plt.plot(epochs_graph, loss, 'b')\n",
    "            plt.plot(epochs_graph, val_loss, 'r')\n",
    "            plt.ylim([0, 2])\n",
    "            ax.set_ylabel('loss')\n",
    "            ax.set_xlabel('epochs')\n",
    "            ax.legend(['train', 'validation'])\n",
    "            plt.title(f'Train vs. Validation Loss {model_type}')\n",
    "\n",
    "            ax = plt.subplot(132)\n",
    "            plt.plot(epochs_graph, accuracy, 'b')\n",
    "            plt.plot(epochs_graph, val_accuracy, 'r')\n",
    "            plt.ylim([0, 1])\n",
    "            ax.set_ylabel('accuracy')\n",
    "            ax.set_xlabel('epochs')\n",
    "            ax.legend(['train', 'validation'])\n",
    "            plt.title(f'Train vs. Validation Accuracy {model_type}')\n",
    "\n",
    "            ax = plt.subplot(133)\n",
    "            plt.plot(epochs_graph, f1, 'b')\n",
    "            plt.plot(epochs_graph, val_f1, 'r')\n",
    "            plt.ylim([0, 1])\n",
    "            ax.set_ylabel('f1')\n",
    "            ax.set_xlabel('epochs')\n",
    "            ax.legend(['train', 'validation'])\n",
    "            plt.title(f'Train vs. Validation F1-Score {model_type}')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            if logging:\n",
    "                if not os.path.exists(os.path.join(logdir, 'figures')):\n",
    "                    os.makedirs(os.path.join(logdir, 'figures'))\n",
    "                fig.savefig(os.path.join(\n",
    "                    logdir, f'figures/loss_accuracy_training_{model_type}.png'), bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        for model_type in models_list:\n",
    "            for reg_mode in PARAMS['reg_mode']:\n",
    "                mask = (history_df.loc[:, 'model_type'] == model_type) & (\n",
    "                    history_df.loc[:, 'reg_mode'] == reg_mode)\n",
    "\n",
    "                loss_all = history_df.loc[mask, 'loss'].to_list()\n",
    "                loss_prep = [loss_all[i * epochs:(i + 1) * epochs]\n",
    "                             for i in range(folds)]\n",
    "                loss = [np.mean([x[i] for x in loss_prep])\n",
    "                        for i in range(epochs)]\n",
    "\n",
    "                val_loss_all = history_df.loc[mask, 'val_loss'].to_list()\n",
    "                val_loss_prep = [\n",
    "                    val_loss_all[i * epochs:(i + 1) * epochs] for i in range(folds)]\n",
    "                val_loss = [np.mean([x[i] for x in val_loss_prep])\n",
    "                            for i in range(epochs)]\n",
    "\n",
    "                accuracy_all = history_df.loc[mask, 'accuracy'].to_list()\n",
    "                accuracy_prep = [\n",
    "                    accuracy_all[i * epochs:(i + 1) * epochs] for i in range(folds)]\n",
    "                accuracy = [np.mean([x[i] for x in accuracy_prep])\n",
    "                            for i in range(epochs)]\n",
    "\n",
    "                val_accuracy_all = history_df.loc[mask, 'val_accuracy'].to_list(\n",
    "                )\n",
    "                val_accuracy_prep = [val_accuracy_all[i *\n",
    "                                                      epochs:(i + 1) * epochs] for i in range(folds)]\n",
    "                val_accuracy = [np.mean([x[i] for x in val_accuracy_prep])\n",
    "                                for i in range(epochs)]\n",
    "\n",
    "                f1_all = history_df.loc[mask, 'f1'].to_list()\n",
    "                f1_prep = [f1_all[i * epochs:(i + 1) * epochs]\n",
    "                           for i in range(folds)]\n",
    "                f1 = [np.mean([x[i] for x in f1_prep]) for i in range(epochs)]\n",
    "\n",
    "                val_f1_all = history_df.loc[mask, 'val_f1'].to_list()\n",
    "                val_f1_prep = [val_f1_all[i * epochs:(i + 1) * epochs]\n",
    "                               for i in range(folds)]\n",
    "                val_f1 = [np.mean([x[i] for x in val_f1_prep])\n",
    "                          for i in range(epochs)]\n",
    "\n",
    "                fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "                ax = plt.subplot(131)\n",
    "                plt.plot(epochs_graph, loss, 'b')\n",
    "                plt.plot(epochs_graph, val_loss, 'r')\n",
    "                plt.ylim([0, 2])\n",
    "                ax.set_ylabel('loss')\n",
    "                ax.set_xlabel('epochs')\n",
    "                ax.legend(['train', 'validation'])\n",
    "                plt.title(f'Train vs. Validation Loss {model_type} {reg_mode}')\n",
    "\n",
    "                ax = plt.subplot(132)\n",
    "                plt.plot(epochs_graph, accuracy, 'b')\n",
    "                plt.plot(epochs_graph, val_accuracy, 'r')\n",
    "                plt.ylim([0, 1])\n",
    "                ax.set_ylabel('accuracy')\n",
    "                ax.set_xlabel('epochs')\n",
    "                ax.legend(['train', 'validation'])\n",
    "                plt.title(\n",
    "                    f'Train vs. Validation Accuracy {model_type} {reg_mode}')\n",
    "\n",
    "                ax = plt.subplot(133)\n",
    "                plt.plot(epochs_graph, f1, 'b')\n",
    "                plt.plot(epochs_graph, val_f1, 'r')\n",
    "                plt.ylim([0, 1])\n",
    "                ax.set_ylabel('f1')\n",
    "                ax.set_xlabel('epochs')\n",
    "                ax.legend(['train', 'validation'])\n",
    "                plt.title(\n",
    "                    f'Train vs. Validation F1-Score {model_type} {reg_mode}')\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "                if logging:\n",
    "                    if not os.path.exists(os.path.join(logdir, 'figures')):\n",
    "                        os.makedirs(os.path.join(logdir, 'figures'))\n",
    "                    fig.savefig(os.path.join(\n",
    "                        logdir, f'figures/loss_accuracy_training_{model_type}_{reg_mode}.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_vs_validation(PARAMS['model_type'], history_all_df, PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Two models were found, which have very good performance on the training data, i.e. they are very well optimised: bag-of-words (=bow) and \"homegrown\" embeddings (=fc_emb)\n",
    "* With this the goal to find the best performance on the training data was achieved. \n",
    "* However, as a result they overfit, i.e. they do not perform well on data they have never seen before. The wide gap between loss vs. validation loss on the graphs above is an indicator of this. \n",
    "* Now, the models should be adjusted to perform well also on unseen data, i.e. to generalise better. \n",
    "* Several possibilites to do that - more data and / or modulate the quantity of information that the model is allowed to store or to add constraints on what information it’s allowed to store. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most prominent patterns, which have a better chance of generalizing well.\n",
    "* The latter is called regularisation and this is what will be applied, as currently there is no additional data available. \n",
    "* Regularisation can be achieved by reducing the capacity of the model, adding weight regularisation or introducing dropout\n",
    "* As both models are not that complex, their capacity will not be reduced\n",
    "* Weights regularisation, Dropout & their combination will be tested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of regularisation as given in \"Deep Learning with Python\" by Chollet, p.130:** \"You may be familiar with the principle of Occam’s razor: given two explanations for something, the explanation most likely to be correct is the simplest one - the one that makes fewer assumptions. This idea also applies to the models learned by neural networks: given some training data and a network architecture, multiple sets of weight values (multiple models) could explain the data. Simpler models are less likely to overfit than complex ones.\n",
    "A simple model in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters, as you saw in the previous section). Thus a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights to take only small values, which makes the distribution of weight values more regular. This is called weight regularization, and it’s done by adding to the loss function of the network a cost associated with having large weights. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of dropout as given in \"Deep Learning with Python\" by Chollet, p.132:** \"...Dropout, applied to a layer, consists of randomly dropping out (setting to zero) a number of output features of the layer during training. Let’s say a given layer would normally return a vector [0.2, 0.5, 1.3, 0.8, 1.1] for a given input sample during training. After applying dropout, this vector will have a few zero entries distributed at random: for example, [0, 0.5, 1.3, 0, 1.1]. The dropout rate is the fraction of the features that are zeroed out; it’s usually set between 0.2 and 0.5. At test time, no units are dropped out; instead, the layer’s output values are scaled down by a factor equal to the dropout rate, to balance for the fact that more units are active than at training time. \n",
    "\n",
    "...was inspired by, among other things, a fraud-prevention mecha- nism used by banks. In his own words, “I went to my bank. The tellers kept changing and I asked one of them why. He said he didn’t know but they got moved around a lot. I figured it must be because it would require cooperation between employees to successfully defraud the bank. This made me realize that randomly removing a different subset of neurons on each example would prevent conspiracies and thus reduce overfitting.” The core idea is that introducing noise in the output values of a layer can break up happenstance patterns that aren’t significant (what Hinton refers to as conspiracies), which the network will start memorizing if no noise is present.\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_reg(model_type, params_dict, reg_mode):\n",
    "    \"\"\"\n",
    "    This function compiles a model according to the chosen model_type\n",
    "    Args: \n",
    "        - model_type - which model type is to be used\n",
    "        - params_dict - dictionary with parameters for the model\n",
    "        - reg_mode - regularisation type applied\n",
    "\n",
    "    Returns: \n",
    "        - a compiled model\n",
    "    \"\"\"\n",
    "\n",
    "    # set parameters\n",
    "    hidden_units = params_dict['hidden_units']\n",
    "    dimensions_labels = params_dict['number_of_classes']\n",
    "    max_words = params_dict['max_words']\n",
    "    max_len = params_dict['max_len']\n",
    "    embedding_size = params_dict['embedding_size']\n",
    "    embedding_dim = params_dict['embedding_dim']\n",
    "    conv_window = params_dict['conv_window']\n",
    "\n",
    "    if reg_mode == 'l2':\n",
    "        # every coefficient in the weight matrix of the layer will add regularisation factor * weight_coefficient_value\n",
    "        # to the total loss of the network\n",
    "        # the default value for the regularisation factor is 0.01\n",
    "        # Keras documentation: https://keras.io/api/layers/regularizers/\n",
    "\n",
    "        # fully connected bag-of-words\n",
    "        if model_type == params_dict['model_type'][0]:\n",
    "            model = models.Sequential()\n",
    "            model.add(layers.Dense(hidden_units,\n",
    "                                   kernel_regularizer=regularizers.l2(),\n",
    "                                   activation='relu',\n",
    "                                   input_shape=(max_words, )))\n",
    "            model.add(layers.Dense(hidden_units,\n",
    "                                   kernel_regularizer=regularizers.l2(),\n",
    "                                   activation='relu'))\n",
    "            model.add(layers.Dense(dimensions_labels, activation='softmax'))\n",
    "\n",
    "        # fully connected & \"homegrown\" embeddings layer\n",
    "        elif model_type == params_dict['model_type'][1]:\n",
    "            model = models.Sequential()\n",
    "            model.add(layers.Embedding(max_words+1,\n",
    "                                       embedding_size, input_length=max_len))\n",
    "            model.add(layers.Flatten())\n",
    "            model.add(layers.Dense(hidden_units,\n",
    "                                   kernel_regularizer=regularizers.l2(),\n",
    "                                   activation='relu'))\n",
    "            model.add(layers.Dense(hidden_units,\n",
    "                                   kernel_regularizer=regularizers.l2(),\n",
    "                                   activation='relu'))\n",
    "            model.add(layers.Dense(dimensions_labels, activation='softmax'))\n",
    "\n",
    "    elif reg_mode == 'dropout':\n",
    "        # rate: what percentage of the input units to drop\n",
    "        # Keras documentation: https://keras.io/api/layers/regularization_layers/dropout/\n",
    "\n",
    "        # fully connected bag-of-words\n",
    "        if model_type == params_dict['model_type'][0]:\n",
    "            model = models.Sequential()\n",
    "            model.add(layers.Dense(hidden_units, activation='relu',\n",
    "                                   input_shape=(max_words, )))\n",
    "            model.add(layers.Dropout(0.5))\n",
    "            model.add(layers.Dense(hidden_units, activation='relu'))\n",
    "            model.add(layers.Dropout(0.5))\n",
    "            model.add(layers.Dense(dimensions_labels, activation='softmax'))\n",
    "\n",
    "        # fully connected & \"homegrown\" embeddings layer\n",
    "        elif model_type == params_dict['model_type'][1]:\n",
    "            model = models.Sequential()\n",
    "            model.add(layers.Embedding(max_words+1,\n",
    "                                       embedding_size, input_length=max_len))\n",
    "            model.add(layers.Flatten())\n",
    "            model.add(layers.Dense(hidden_units, activation='relu'))\n",
    "            model.add(layers.Dropout(0.5))\n",
    "            model.add(layers.Dense(hidden_units, activation='relu'))\n",
    "            model.add(layers.Dropout(0.5))\n",
    "            model.add(layers.Dense(dimensions_labels, activation='softmax'))\n",
    "\n",
    "    elif reg_mode == 'combi':\n",
    "        # fully connected bag-of-words\n",
    "        if model_type == params_dict['model_type'][0]:\n",
    "            model = models.Sequential()\n",
    "            model.add(layers.Dense(hidden_units,\n",
    "                                   kernel_regularizer=regularizers.l2(),\n",
    "                                   activation='relu',\n",
    "                                   input_shape=(max_words, )))\n",
    "            model.add(layers.Dropout(0.5))\n",
    "            model.add(layers.Dense(hidden_units,\n",
    "                                   kernel_regularizer=regularizers.l2(),\n",
    "                                   activation='relu'))\n",
    "            model.add(layers.Dropout(0.5))\n",
    "            model.add(layers.Dense(dimensions_labels, activation='softmax'))\n",
    "\n",
    "        # fully connected & \"homegrown\" embeddings layer\n",
    "        elif model_type == params_dict['model_type'][1]:\n",
    "            model = models.Sequential()\n",
    "            model.add(layers.Embedding(max_words+1,\n",
    "                                       embedding_size, input_length=max_len))\n",
    "            model.add(layers.Flatten())\n",
    "            model.add(layers.Dense(hidden_units,\n",
    "                                   kernel_regularizer=regularizers.l2(),\n",
    "                                   activation='relu'))\n",
    "            model.add(layers.Dropout(0.5))\n",
    "            model.add(layers.Dense(hidden_units,\n",
    "                                   kernel_regularizer=regularizers.l2(),\n",
    "                                   activation='relu'))\n",
    "            model.add(layers.Dropout(0.5))\n",
    "            model.add(layers.Dense(dimensions_labels, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type in ['bow', 'fc_emb']:\n",
    "    for reg_mode in PARAMS['reg_mode']:\n",
    "        model = generate_model_reg(model_type, PARAMS, reg_mode)\n",
    "\n",
    "        if model_type == PARAMS['model_type'][0]:\n",
    "            train_with_cross_val(PARAMS['k'], model, model_type, x_train_oh,\n",
    "                                 y_train_oh, PARAMS['epochs'], PARAMS['batch_size'], reg_mode, reg=True)\n",
    "        else:\n",
    "            train_with_cross_val(\n",
    "                PARAMS['k'], model, model_type, x_train, y_train, PARAMS['epochs'], PARAMS['batch_size'], reg_mode, reg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_all_df_reg = pd.DataFrame()\n",
    "\n",
    "for model_type in ['bow', 'fc_emb']:\n",
    "    for reg_mode in PARAMS['reg_mode']:\n",
    "        for i in range(PARAMS['k']):\n",
    "            history = np.load(\n",
    "                f'history_{model_type}_{reg_mode}_fold_{i}.npy', allow_pickle='TRUE').item()\n",
    "            history['model_type'] = model_type\n",
    "            history['reg_mode'] = reg_mode\n",
    "\n",
    "            history_df_reg = pd.DataFrame(history)\n",
    "\n",
    "            history_df_reg['f1'] = 2 * history_df.precision * \\\n",
    "                history_df.recall / \\\n",
    "                (history_df_reg.precision + history_df_reg.recall)\n",
    "            history_df_reg['f1'] = history_df_reg['f1'].fillna(0)\n",
    "            history_df_reg['val_f1'] = 2 * history_df_reg.val_precision * \\\n",
    "                history_df_reg.val_recall / \\\n",
    "                (history_df_reg.val_precision + history_df_reg.val_recall)\n",
    "            history_df_reg['val_f1'] = history_df_reg['val_f1'].fillna(0)\n",
    "\n",
    "            history_all_df_reg = history_all_df_reg.append(history_df_reg)\n",
    "\n",
    "history_all_df_reg\n",
    "history_all_df_pivot_reg = history_all_df_reg.pivot_table(columns=['model_type', 'reg_mode'],\n",
    "                                                          values=['loss', 'val_loss', 'accuracy', 'val_accuracy', 'precision', 'val_precision', 'f1', 'val_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_all_df_pivot_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_of_regularised_models = PARAMS['model_type'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_vs_validation(choice_of_regularised_models,\n",
    "                         history_all_df_reg, PARAMS, reg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the No Regularised Models to the Regularised Ones to Determine the Optimal One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspired by: https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_by_metric(model_type, reg_mode, history_all_df, history_all_df_reg, metric_train, metric_val, params):\n",
    "    '''\n",
    "    Function that plots a training and a validation metric for a non-regularised and regularised model \n",
    "\n",
    "    Args:\n",
    "        - model_type - model used\n",
    "        - reg_mode - regularisation mode used - l2, dropout, combination of the two\n",
    "        - history_all_df - dataframe with results from models without regularisation\n",
    "        - history_all_df_reg - dataframe with results from models with regularisation\n",
    "        - metric_train - training metric to compare\n",
    "        - metric_val - validation metric to compare\n",
    "        - params - parameters dictionary\n",
    "    '''\n",
    "\n",
    "    epochs = params['epochs']\n",
    "    epochs_graph = range(1, params['epochs'] + 1)\n",
    "    folds = params['k']\n",
    "\n",
    "    mask_base = (history_all_df.loc[:, 'model_type'] == model_type)\n",
    "    mask_reg = (history_all_df_reg.loc[:, 'model_type'] == model_type) & (\n",
    "        history_all_df_reg.loc[:, 'reg_mode'] == reg_mode)\n",
    "\n",
    "    # metric train base model\n",
    "    metric_train_all_base = history_all_df.loc[mask_base, metric_train].to_list(\n",
    "    )\n",
    "    metric_train_prep_base = [\n",
    "        metric_train_all_base[i * epochs:(i + 1) * epochs] for i in range(folds)]\n",
    "    metric_train_base = [np.mean([x[i] for x in metric_train_prep_base])\n",
    "                         for i in range(epochs)]\n",
    "\n",
    "    # metric train regularised model\n",
    "    metric_train_all_reg = history_all_df_reg.loc[mask_reg, metric_train].to_list(\n",
    "    )\n",
    "    metric_train_prep_reg = [\n",
    "        metric_train_all_reg[i * epochs:(i + 1) * epochs] for i in range(folds)]\n",
    "    metric_train_reg = [np.mean([x[i] for x in metric_train_prep_reg])\n",
    "                        for i in range(epochs)]\n",
    "\n",
    "    # metric validation base model\n",
    "    metric_val_all_base = history_all_df.loc[mask_base, metric_val].to_list()\n",
    "    metric_val_prep_base = [\n",
    "        metric_val_all_base[i * epochs:(i + 1) * epochs] for i in range(folds)]\n",
    "    metric_val_base = [np.mean([x[i] for x in metric_val_prep_base])\n",
    "                       for i in range(epochs)]\n",
    "\n",
    "    # metric validation regularised model\n",
    "    metric_val_all_reg = history_all_df_reg.loc[mask_reg, metric_val].to_list()\n",
    "    metric_val_prep_reg = [\n",
    "        metric_val_all_reg[i * epochs:(i + 1) * epochs] for i in range(folds)]\n",
    "    metric_val_reg = [np.mean([x[i] for x in metric_val_prep_reg])\n",
    "                      for i in range(epochs)]\n",
    "\n",
    "    metrics_dict = {\n",
    "        'loss': 'Training Loss',\n",
    "        'acc': 'Training Accuracy',\n",
    "        'precision': 'Training Precision',\n",
    "        'recall': 'Training Recall',\n",
    "        'f1': 'Training F1',\n",
    "        'val_loss': 'Validation Loss',\n",
    "        'val_acc': 'Validation Accuracy',\n",
    "        'val_precision': 'Validation Precision',\n",
    "        'val_recall': 'Validation Recall',\n",
    "        'val_f1': 'Validation F1'\n",
    "    }\n",
    "\n",
    "    metric_train_label = metrics_dict[metric_train]\n",
    "    metric_val_label = metrics_dict[metric_val]\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "    plt.plot(epochs_graph, metric_train_base, 'b',\n",
    "             label=f'{model_type} No Regularisation Training Loss')\n",
    "    plt.plot(epochs_graph, metric_train_reg, 'bo',\n",
    "             label=f'{model_type} {reg_mode} Regularisation Training Loss')\n",
    "    plt.plot(epochs_graph, metric_val_base, 'r',\n",
    "             label=f'{model_type} No Regularisation Validation Loss')\n",
    "    plt.plot(epochs_graph, metric_val_reg, 'ro',\n",
    "             label=f'{model_type} {reg_mode} Regularisation Validation Loss')\n",
    "    plt.xlabel('Epoch number')\n",
    "    plt.ylabel(f'{metric_train_label}, {metric_val_label}')\n",
    "    plt.ylim([0, 2])\n",
    "    plt.title(\n",
    "        f'Comparing {metric_train_label} & {metric_val_label}es - base vs. {reg_mode} {model_type} model')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    if logging:\n",
    "        if not os.path.exists(os.path.join(logdir, 'figures')):\n",
    "            os.makedirs(os.path.join(logdir, 'figures'))\n",
    "        fig.savefig(os.path.join(\n",
    "            logdir, f'figures/compare_base_vs_regularised_model_{model_type}_{reg_mode}.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs below help to analyse 3 things pointing to a successful reduction of the overfitting: \n",
    "* narrower gap between training and validataion loss\n",
    "* does the regularised model overfit \"later\" than the baseline one, i.e. it takes more epochs before train & validation loss go apart.\n",
    "* does the validation loss of the regularised model goes up slower than the one of the baseline\n",
    "\n",
    "Results for **l2**:\n",
    "- for the bow model - the gap seems about the same. The overfitting starts at the same time as the baseline model. However, the validation loss increases slower afterward. \n",
    "- the regularised \"homegrown\"-embedding model exhibits also about the same gap as compared to the baseline model and a shift in the epoch when the overfitting starts. At the same time there is a slightly smoother progress of the regularised validation loss.\n",
    "\n",
    "\n",
    "Results for **dropout**:\n",
    "- bow - narrower gap; light shift in the epoch; smoother validation loss development\n",
    "- \"homegrown\"-embedding model - wider gap; shift in the epoch, steeper regularised validation loss\n",
    "\n",
    "\n",
    "Results for **combi** = l2 & dropout:\n",
    "- bow - narrower gap; no shift; smoother validation loss progress\n",
    "- \"homegrown\"-embedding model - narrower gap; significant shift; significantly smoother development of the regularised validation loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above the bow model with dropout performs the best - least overfitting (narrowest gap between training and validation loss), lowest loss of all regularised models and smooth regularised validation loss development. \n",
    "\n",
    "The second best model is the \"homegrown\"-embedding one, as the overfitting is significantly reduced and the regularised validation loss is stable. \n",
    "\n",
    "The final decision which model to use will be made based on: \n",
    "- F1 metric and \n",
    "- the performance on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type in ['bow', 'fc_emb']:\n",
    "    for reg_mode in PARAMS['reg_mode']:\n",
    "        compare_models_by_metric(\n",
    "            model_type, reg_mode, history_all_df, history_all_df_reg, 'loss', 'val_loss', PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Validation F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here bag-of-words with dropout is again the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type in ['bow', 'fc_emb']:\n",
    "    for reg_mode in PARAMS['reg_mode']:\n",
    "        mask_model_reg = (history_all_df_reg.loc[:, 'model_type'] == model_type) & (\n",
    "            history_all_df_reg.loc[:, 'reg_mode'] == reg_mode)\n",
    "        f1_model_reg = np.mean(\n",
    "            history_all_df_reg.loc[mask_model_reg, 'val_f1'])\n",
    "        print(\n",
    "            f'{model_type} with {reg_mode} regularisation has F1 score of {f1_model_reg:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation of Models on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-words with dropout is again the best model when it comes to F1. However, the loss is quite high. The model with both l2 and dropout has a F1 score only slightly lower than the model only with dropout, but has a much lower loss. Therefore, the former will be selected as the final model. It is the best model to generalise. \n",
    "\n",
    "Below I also offer a few thoughts what were my expectations at the beginning and why the data proved otherwise: \n",
    "\n",
    "* my expectations were that a model with pre-trained embeddings in combination with an LSTM or Conv1D will outperform the rest. \n",
    "* My reasoning was that the pre-trained embeddings will enforce the generalisation while an LSTM or Conv1D will help capture the text sequence. \n",
    "\n",
    "However, it turned out: \n",
    "* that \"homegrown\" embeddings are better than pre-trained. This can be explained by the small amount of data and the tech concentrated nature of the text descriptions. These \"specificities\" were better captured by training the data's own embeddings (that is why also \"homegrown\"). \n",
    "* that bag-of-words is better than any model with embeddings. This for me was the biggest surprise. After a discussion with a mentor of mine an explanation can be that due to the short length of the text descriptions and relatively \"loose\" way of writing them, semantics and text structure do not play a significant role. Much more important is, if a word is present, which is what bag-of-words captures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_type, reg_mode, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Function that loads the saved trained models and evaluates on the test data\n",
    "\n",
    "    Args: \n",
    "        - model_type - model to be used\n",
    "        - reg_mode - regulasation applied\n",
    "        - x_test - test features (= descriptions)\n",
    "        - y_test - test labels (= sentiments)\n",
    "\n",
    "    Returns: \n",
    "        - results - loss, accuracy, recall, precision\n",
    "        - f1_score  \n",
    "    \"\"\"\n",
    "\n",
    "    model_path = os.path.join(logdir, f'{model_type}_{reg_mode}')\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    results = model.evaluate(x_test, y_test)\n",
    "    precision = results[1]\n",
    "    recall = results[2]\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "    print()\n",
    "    print(f'Test f1-score for {model_type} with {reg_mode}: {f1_score}')\n",
    "    return results, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type in ['bow', 'fc_emb']:\n",
    "    for reg_mode in PARAMS['reg_mode']:\n",
    "        if model_type == 'bow':\n",
    "            results, f1_score = test_model(\n",
    "                model_type, reg_mode, x_test_oh, y_test_oh)\n",
    "        else:\n",
    "            results, f1_score = test_model(\n",
    "                model_type, reg_mode, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a look at the confusion matrix in order to see how the models perform on the different classes. \n",
    "\n",
    "The matrixes confirm the results from the evaluation on the test data - the best model is now bag-of-words with both l2 and dropout. Compared to the other models it performs (relatively) well among all classes (= the sentiments) in terms of accuracy. \n",
    "\n",
    "Source: https://github.com/javaidnabi31/Multi-class-with-imbalanced-dataset-classification/blob/master/20-news-group-classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, params,\n",
    "                          normalise=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Args: \n",
    "        - cm - confusion matrix as generated by sklearns confusion_matrix\n",
    "        - classes - labels (=sentiments)\n",
    "        - normalise - normalisation can be applied by setting `normalise=True`\n",
    "        - title of the plot\n",
    "        - cmap - colors of the label squares\n",
    "    \"\"\"\n",
    "    if normalise:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalised confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalisation')\n",
    "\n",
    "    number_of_classes = params['number_of_classes']\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(number_of_classes)\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalise else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_descr.loc[:, 'Sentiment'].unique()\n",
    "classes = classes.sort()\n",
    "\n",
    "for model_type in ['bow', 'fc_emb']:\n",
    "    for reg_mode in PARAMS['reg_mode']:\n",
    "        model_path = os.path.join(logdir, f'{model_type}_{reg_mode}')\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "        if model_type == 'bow':\n",
    "            y_pred = model.predict(x_test_oh)\n",
    "            cnf_matrix = confusion_matrix(\n",
    "                np.argmax(y_test_oh, axis=1), np.argmax(y_pred, axis=1))\n",
    "\n",
    "            # Plot normalised confusion matrix\n",
    "            fig = plt.figure()\n",
    "            fig.set_size_inches(7, 6, forward=True)\n",
    "\n",
    "            plot_confusion_matrix(cnf_matrix, classes, PARAMS, normalise=True,\n",
    "                                  title=f'Normalised confusion matrix of model {model_type} with {reg_mode}')\n",
    "        else:\n",
    "            y_pred = model.predict(x_test)\n",
    "            cnf_matrix = confusion_matrix(\n",
    "                np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
    "\n",
    "            # Plot confusion matrix\n",
    "            fig = plt.figure()\n",
    "            fig.set_size_inches(7, 6, forward=True)\n",
    "\n",
    "            plot_confusion_matrix(cnf_matrix, classes, PARAMS, normalise=True,\n",
    "                                  title=f'Normalised confusion matrix of model {model_type} with {reg_mode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type in PARAMS['model_type']:\n",
    "    model_path = os.path.join(logdir, f'{model_type}')\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    if model_type == 'bow':\n",
    "        y_pred = model.predict(x_test_oh)\n",
    "        cnf_matrix = confusion_matrix(\n",
    "            np.argmax(y_test_oh, axis=1), np.argmax(y_pred, axis=1))\n",
    "\n",
    "        # Plot normalised confusion matrix\n",
    "        fig = plt.figure()\n",
    "        fig.set_size_inches(7, 6, forward=True)\n",
    "\n",
    "        plot_confusion_matrix(cnf_matrix, classes, PARAMS, normalise=False,\n",
    "                              title=f'Normalised confusion matrix of model {model_type}')\n",
    "    else:\n",
    "        y_pred = model.predict(x_test)\n",
    "        cnf_matrix = confusion_matrix(\n",
    "            np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        fig = plt.figure()\n",
    "        fig.set_size_inches(7, 6, forward=True)\n",
    "\n",
    "        plot_confusion_matrix(cnf_matrix, classes, PARAMS, normalise=False,\n",
    "                              title=f'Normalised confusion matrix of model {model_type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On the Side: Defining the Epoch with the Lowest Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_epoch(history_df, model_type, reg=False, reg_mode=None):\n",
    "    \"\"\"\n",
    "    Function to return the epoch number where the validation loss is\n",
    "    at its minimum\n",
    "\n",
    "    Args:\n",
    "        history_df - dataframe with training & validation history of model\n",
    "        model_type - which model\n",
    "        reg - is regularisation applied (default value is False)\n",
    "        reg_mode - if reg==True, which regularisation was applied\n",
    "    Returns:\n",
    "        epoch number with minimum validation loss\n",
    "    \"\"\"\n",
    "\n",
    "    if not reg:\n",
    "        mask = history_df.loc[:, 'model_type'] == model_type\n",
    "        min_epoch = np.argmin(history_df.loc[mask, 'val_loss'])\n",
    "        print(\n",
    "            f'Minimum validation loss for {model_type} reached in epoch {min_epoch}')\n",
    "\n",
    "    else:\n",
    "        mask = (history_df.loc[:, 'model_type'] == model_type) & (\n",
    "            history_df.loc[:, 'reg_mode'] == reg_mode)\n",
    "        min_epoch = np.argmin(history_df.loc[mask, 'val_loss'])\n",
    "        print(\n",
    "            f'Minimum validation loss for {model_type} and reg mode {reg_mode} reached in epoch {min_epoch}')\n",
    "\n",
    "    return min_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type in PARAMS['model_type']:\n",
    "    optimal_epoch(history_all_df, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type in ['bow', 'fc_emb']:\n",
    "    for reg_mode in PARAMS['reg_mode']:\n",
    "        optimal_epoch(history_all_df_reg, model_type,\n",
    "                      reg=True, reg_mode=reg_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-sent",
   "language": "python",
   "name": "nlp-sent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
