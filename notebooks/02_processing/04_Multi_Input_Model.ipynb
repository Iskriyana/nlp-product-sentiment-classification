{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras import Input\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "from keras import utils\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_validate # to run validation on multiple metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folders\n",
    "home = os.getenv(\"HOME\")\n",
    "nlp_repo = os.path.join(home, 'git/nlp-product-sentiment-classification')\n",
    "\n",
    "# data\n",
    "train_csv_path = os.path.join(nlp_repo, 'data/03_processed/Train.csv')\n",
    "train_descr = pd.read_csv(train_csv_path)\n",
    "\n",
    "test_csv_path = os.path.join(nlp_repo, 'data/03_processed/Test.csv')\n",
    "test_descr = pd.read_csv(test_csv_path)\n",
    "\n",
    "# encoded tokens\n",
    "preprocessed_corpus_path_TF = os.path.join(\n",
    "    nlp_repo, 'data/03_processed/product_descr_preprocessed_TF.p')\n",
    "\n",
    "preprocessed_corpus_path_TF_oh = os.path.join(\n",
    "    nlp_repo, 'data/03_processed/product_descr_preprocessed_TF_oh.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging = True\n",
    "\n",
    "saving = True\n",
    "\n",
    "PARAMS = {\n",
    "\n",
    "    # Define experiment name:\n",
    "    'experiment_name': 'all_models_no_counter_overfitting',\n",
    "\n",
    "    # List of models\n",
    "    'model_type': ['bow', 'fc_emb', 'fc_transf', 'lstm', 'lstm_transf', 'conv1d', 'conv1d_transf'],\n",
    "\n",
    "    # Parameters general:\n",
    "    'number_of_classes': len(np.unique(train_descr['Sentiment'])),\n",
    "    'n_splits': 4,\n",
    "    'seed': 42,\n",
    "    'batch_size': 32,  # [16, 32, 64],\n",
    "    'epochs': 15,\n",
    "    'hidden_units': 32,  # [16, 32, 64],\n",
    "\n",
    "    # NLP Parameters\n",
    "    # max_words = vocabulary size = our samples - number of most frequent words.\n",
    "    # I set it to 10.000, although in this case there are less.\n",
    "    # I do this to parametise the code.\n",
    "    # Aleternatively, I can set it to the length of our vocabulary = word_index\n",
    "    'max_words': 10000,\n",
    "\n",
    "    # embedding_size = embedding dimensionality\n",
    "    'embedding_size': 30,  # [10, 20, 30, 100],\n",
    "\n",
    "    # Cross-fold validation:\n",
    "    'k': 4,\n",
    "}\n",
    "\n",
    "logdir = f'logs/experiments/{PARAMS[\"experiment_name\"]}_' + \\\n",
    "    datetime.datetime.now().strftime(\"%Y_%m_%d-%H:%M\")\n",
    "logdir_tb = f'logs/tensorboard/experiments/{PARAMS[\"experiment_name\"]}_' + \\\n",
    "    datetime.datetime.now().strftime(\"%Y_%m_%d-%H:%M\")\n",
    "\n",
    "# create logging folder and tensorboard callback function\n",
    "if logging:\n",
    "    print(f'Log results to {logdir}')\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    tensorboard_callbacks = [tf.keras.callbacks.TensorBoard(log_dir=logdir_tb)]\n",
    "\n",
    "else:\n",
    "    logdir = ''\n",
    "    logdir_tb = ''\n",
    "    tensorboard_callbacks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, labels):\n",
    "    dimensions_labels = PARAMS['number_of_classes']\n",
    "\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    data = data[indices]\n",
    "    labels = np.asarray(labels)\n",
    "    labels = labels[indices]\n",
    "\n",
    "    training_samples = int(0.8 * len(data))\n",
    "\n",
    "    x_train = data[:training_samples]\n",
    "    y_train = labels[:training_samples]\n",
    "\n",
    "    x_test = data[training_samples:]\n",
    "    y_test = labels[training_samples:]\n",
    "\n",
    "    y_train_oh = tf.one_hot(indices=y_train, depth=dimensions_labels)\n",
    "    y_train_oh = np.asarray(y_train_oh)\n",
    "\n",
    "    y_test_oh = tf.one_hot(indices=y_test, depth=dimensions_labels)\n",
    "    y_test_oh = np.asarray(y_test_oh)\n",
    "\n",
    "    return x_train, y_train_oh, x_test, y_test_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = train_test_split(data, labels)\n",
    "\n",
    "x_train_oh, y_train_oh, x_test_oh, y_test_oh = train_test_split(\n",
    "    data_oh, labels_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversample the Minority Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_smote(x_train, y_train):\n",
    "    oversample = SMOTE()\n",
    "    x_train, y_train = oversample.fit_resample(x_train, y_train)\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "def plot_classes(y_train):\n",
    "    counter = Counter(y_train)\n",
    "    for k, v in counter.items():\n",
    "        per = v / len(y_train) * 100\n",
    "        print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n",
    "    # plot the distribution\n",
    "    plt.bar(counter.keys(), counter.values())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the multi-input model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = params_dict['hidden_units']\n",
    "dimensions_labels = params_dict['number_of_classes']\n",
    "max_words = params_dict['max_words']\n",
    "max_len = params_dict['max_len']\n",
    "embedding_size = params_dict['embedding_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Text part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = Input(shape=(None,), dtype='int32', name='text')\n",
    "embedded_text = layers.Embedding(max_words+1, embedding_size, input_length=max_len)(text_input)\n",
    "flatten_text = layers.Flatten()(embedded_text)\n",
    "encoded_text_1 = layers.Dense(hidden_units, activation='relu')(flatten_text)\n",
    "encoded_text_2 = layers.Dense(hidden_units, activation='relu')(encoded_text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-sent",
   "language": "python",
   "name": "nlp-sent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
