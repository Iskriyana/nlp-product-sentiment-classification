{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import utils\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folders\n",
    "home = os.getenv(\"HOME\")\n",
    "nlp_repo = os.path.join(home, 'git/nlp-product-sentiment-classification')\n",
    "\n",
    "# data\n",
    "train_csv_path = os.path.join(nlp_repo, 'data/03_processed/Train.csv')\n",
    "train_descr = pd.read_csv(train_csv_path)\n",
    "\n",
    "test_csv_path = os.path.join(nlp_repo, 'data/03_processed/Test.csv')\n",
    "test_descr = pd.read_csv(test_csv_path)\n",
    "\n",
    "# encoded tokens\n",
    "preprocessed_corpus_path_TF = os.path.join(\n",
    "    nlp_repo, 'data/03_processed/product_descr_preprocessed_TF.p')\n",
    "\n",
    "preprocessed_corpus_path_TF_oh = os.path.join(\n",
    "    nlp_repo, 'data/03_processed/product_descr_preprocessed_TF_oh.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read-in the list of tokens and the vocabulary\n",
    "sequences, word_index = pd.read_pickle(preprocessed_corpus_path_TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Tokens\n",
    "\n",
    "# max_words = vocabulary size = our samples - number of most frequent words.\n",
    "# We set it to 10.000, although in our particular case we have less.\n",
    "# We do this to parametise the code.\n",
    "# Aleternatively, we can set it to the length of our vocabulary = word_index\n",
    "max_words = 10000\n",
    "\n",
    "# embedding_size = embedding dimensionality\n",
    "embedding_size = 10\n",
    "\n",
    "# max_len = sequence length - the text is cut off after this number of words\n",
    "# in our case we define it as the maximum sequence length in our list of tokenised sequences\n",
    "max_len = np.max([len(x) for x in sequences])\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "training_samples = int(0.8 * len(train_descr['Product_Description']))\n",
    "model_path = \"product_descr_TF.h5\"\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# For GloVe word-embeddings matrix (pre-trained model)\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_descr.loc[:, 'Sentiment'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions_labels = len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_encoded = tf.one_hot(indices=labels, depth=dimensions_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_encoded = np.asarray(labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor', labels_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[indices]\n",
    "labels_encoded = labels_encoded[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:training_samples]\n",
    "y_train = labels_encoded[:training_samples]\n",
    "\n",
    "x_val = data[training_samples:]\n",
    "y_val = labels_encoded[training_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(my_dict, val):\n",
    "    for key, value in my_dict.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "\n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_key(word_index, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bag-of-words will be used as a baseline model. \n",
    "* Its advantages are that it is fairly easy and quick to build. \n",
    "* The downside is that bag-of-words does not perserve the order of the sentence. As a result the structure of a sentence is lost. \n",
    "* RNNs & 1D ConvNets will be tried later in order to remedy this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_oh, word_index_oh = pd.read_pickle(preprocessed_corpus_path_TF_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_oh = train_descr.loc[:, 'Sentiment'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_oh = sequences_oh[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_oh = data_oh[:training_samples]\n",
    "y_train_oh = y_train\n",
    "\n",
    "x_val_oh = data_oh[training_samples:]\n",
    "y_val_oh = y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected NN & Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bow = models.Sequential()\n",
    "\n",
    "model_bow.add(layers.Dense(16, activation='relu',\n",
    "                       input_shape=(max_words, )))\n",
    "model_bow.add(layers.Dropout(0.75))\n",
    "model_bow.add(layers.Dense(16, activation='relu'))\n",
    "model_bow.add(layers.Dropout(0.75))\n",
    "model_bow.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "model_bow.summary()\n",
    "\n",
    "model_bow.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_bow = model_bow.fit(\n",
    "    x_train_oh, y_train_oh,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val_oh, y_val_oh)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_bow = history_bow.history['loss']\n",
    "val_loss_bow = history_bow.history['val_loss']\n",
    "acc_bow = history_bow.history['accuracy']\n",
    "val_acc_bow = history_bow.history['val_accuracy']\n",
    "\n",
    "\n",
    "epochs = range(1, len(acc_bow) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_bow, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_bow, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc_bow, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_bow, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Conneceted NN & Embeddings Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emb = models.Sequential()\n",
    "\n",
    "model_emb.add(layers.Embedding(max_words+1,\n",
    "                               embedding_size, input_length=max_len))\n",
    "model_emb.add(layers.Flatten())\n",
    "model_emb.add(layers.Dense(32, activation='relu'))\n",
    "model_emb.add(layers.Dropout(0.75))\n",
    "model_emb.add(layers.Dense(32, activation='relu'))\n",
    "model_emb.add(layers.Dropout(0.75))\n",
    "model_emb.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "model_emb.summary()\n",
    "\n",
    "model_emb.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_emb = model_emb.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_emb = history_emb.history['loss']\n",
    "val_loss_emb = history_emb.history['val_loss']\n",
    "acc_emb = history_emb.history['accuracy']\n",
    "val_acc_emb = history_emb.history['val_accuracy']\n",
    "\n",
    "\n",
    "epochs = range(1, len(acc_emb) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_emb, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_emb, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc_emb, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_emb, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Conneceted NN & Pre-Trained Embeddings Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = './glove.6B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((max_words+1, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transf = models.Sequential()\n",
    "\n",
    "model_transf.add(layers.Embedding(max_words+1,\n",
    "                                  embedding_dim, input_length=max_len))\n",
    "\n",
    "model_transf.add(layers.Flatten())\n",
    "model_transf.add(layers.Dense(16, activation='relu'))\n",
    "model_transf.add(layers.Dropout(0.75))\n",
    "model_transf.add(layers.Dense(16, activation='relu'))\n",
    "model_transf.add(layers.Dropout(0.75))\n",
    "model_transf.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "model_transf.layers[0].set_weights([embedding_matrix])  # !!!! important !!!!\n",
    "model_transf.layers[0].trainable = False\n",
    "\n",
    "\n",
    "# needed in order to save the weights: https://stackoverflow.com/questions/55908188/this-model-has-not-yet-been-built-error-on-model-summary\n",
    "model_transf.build((max_len,))\n",
    "\n",
    "model_transf.summary()\n",
    "\n",
    "\n",
    "model_transf.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_transf = model_transf.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val)\n",
    ")\n",
    "\n",
    "model_transf.save_weights(f'{model_path}_transf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_transf = history_transf.history['loss']\n",
    "val_loss_transf = history_transf.history['val_loss']\n",
    "acc_transf = history_transf.history['accuracy']\n",
    "val_acc_transf = history_transf.history['val_accuracy']\n",
    "\n",
    "\n",
    "epochs = range(1, len(acc_transf) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_transf, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_transf, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc_transf, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_transf, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = models.Sequential()\n",
    "\n",
    "model_lstm.add(layers.Embedding(max_words+1,\n",
    "                                embedding_dim, input_length=max_len))\n",
    "\n",
    "# model_lstm.add(layers.Flatten())\n",
    "# Option 1\n",
    "model_lstm.add(layers.LSTM(32))\n",
    "\n",
    "# Option 2\n",
    "# model.add(layers.LSTM(32, return_sequences=True)) # stacking of LSTMs\n",
    "# model.add(layers.LSTM(32)) # the size of the hidden state, randomly defined\n",
    "model_lstm.add(layers.Dense(16, activation='relu'))\n",
    "model_lstm.add(layers.Dropout(0.5))\n",
    "model_lstm.add(layers.Dense(16, activation='relu'))\n",
    "model_lstm.add(layers.Dropout(0.5))\n",
    "model_lstm.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "model_lstm.layers[0].set_weights([embedding_matrix])  # !!!! important !!!!\n",
    "model_lstm.layers[0].trainable = False\n",
    "\n",
    "model_lstm.summary()\n",
    "\n",
    "model_lstm.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_lstm = model_lstm.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_lstm = history_lstm.history['loss']\n",
    "val_loss_lstm = history_lstm.history['val_loss']\n",
    "acc_lstm = history_lstm.history['accuracy']\n",
    "val_acc_lstm = history_lstm.history['val_accuracy']\n",
    "\n",
    "\n",
    "epochs = range(1, len(acc_lstm) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_lstm, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_lstm, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc_lstm, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_lstm, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Convolutional NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1d = models.Sequential()\n",
    "\n",
    "model_conv1d.add(layers.Embedding(max_words+1,\n",
    "                                  embedding_dim, input_length=max_len))\n",
    "\n",
    "model_conv1d.add(layers.Conv1D(\n",
    "    32,  # features to be extracted\n",
    "    5,  # convolutional window size\n",
    "    activation='relu',\n",
    "))\n",
    "model_conv1d.add(layers.MaxPooling1D(3))\n",
    "model_conv1d.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model_conv1d.add(layers.GlobalMaxPooling1D())\n",
    "model_conv1d.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "model_conv1d.layers[0].set_weights([embedding_matrix])  # !!!! important !!!!\n",
    "model_conv1d.layers[0].trainable = False\n",
    "\n",
    "model_conv1d.summary()\n",
    "\n",
    "model_conv1d.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_conv1d = model_conv1d.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_conv1d = history_conv1d.history['loss']\n",
    "val_loss_conv1d = history_conv1d.history['val_loss']\n",
    "acc_conv1d = history_conv1d.history['accuracy']\n",
    "val_acc_conv1d = history_conv1d.history['val_accuracy']\n",
    "\n",
    "\n",
    "epochs = range(1, len(acc_conv1d) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_conv1d, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_conv1d, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc_conv1d, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_conv1d, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-sent",
   "language": "python",
   "name": "nlp-sent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
