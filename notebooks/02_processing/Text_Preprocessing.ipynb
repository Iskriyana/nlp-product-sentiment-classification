{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/iskriyanavasileva/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /Users/iskriyanavasileva/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import sys\n",
    "import urllib # check\n",
    "from collections import Counter # check\n",
    "import html\n",
    "import nltk \n",
    "nltk.download('punkt') # check\n",
    "nltk.download('perluniprops') # check\n",
    "from nltk import word_tokenize\n",
    "import pickle\n",
    "import random # check\n",
    "import progressbar\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import utils\n",
    "\n",
    "# This tokenizer is nice, but could cause problems.\n",
    "try:\n",
    "    from nltk.tokenize.moses import MosesDetokenizer\n",
    "    detokenizer = MosesDetokenizer()\n",
    "    use_moses_detokenizer = True\n",
    "except:\n",
    "    use_moses_detokenizer = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus parameters\n",
    "#corpus_path = \n",
    "\n",
    "# Preprocessing parameters\n",
    "preprocessed_corpus_path = \"product_descr_preprocessed.p\"\n",
    "most_common_words_number = 10000\n",
    "\n",
    "# Training parameters\n",
    "train_anyway = False\n",
    "model_path = \"product_descr.h5\"\n",
    "#dataset_size = 5000\n",
    "sequence_length = 30\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "hidden_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequence(sequence, vocabulary):\n",
    "    \"\"\" Encodes a sequence of tokens into a sequence of indices. \"\"\"\n",
    "\n",
    "    return [vocabulary.index(element) for element in sequence if element in vocabulary]\n",
    "\n",
    "\n",
    "def decode_indices(indices, vocabulary):\n",
    "    \"\"\" Decodes a sequence of indices and returns a string. \"\"\"\n",
    "\n",
    "    decoded_tokens = [vocabulary[index] for index in indices]\n",
    "    if use_moses_detokenizer  == True:\n",
    "        return detokenizer.detokenize(decoded_tokens, return_str=True)\n",
    "    else:\n",
    "        return \" \".join(decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.getenv(\"HOME\")\n",
    "nlp_repo = os.path.join(home, 'git/nlp-product-sentiment-classification')\n",
    "src_dir = os.path.join(os.getcwd(), '..', 'src')\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = os.path.join(nlp_repo, 'data/03_processed/Train.csv')\n",
    "train_descr = pd.read_csv(train_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The Web Designer\\x89Ûªs Guide to iOS (and Android) Apps, today @mention 10 a.m! {link} #sxsw',\n",
       "       'RT @mention Line for iPad 2 is longer today than yesterday. #SXSW  // are you getting in line again today just for fun?',\n",
       "       'Crazy that Apple is opening a temporary store in Austin tomorrow to handle the rabid #sxsw eye pad too seekers.',\n",
       "       ...,\n",
       "       'RT @mention RT @mention Download 20+ free tracks from @mention Music Sampler @mention including @glove! {link} #SXSW',\n",
       "       \"OH at Texas Social Media Awards: 'You don't need to ask your mother anymore. Just Google it.' #sxswi #sxsw\",\n",
       "       '#Google launching a &quot;major&quot; new social network at #sxsw ... Wonder what that can be...'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_descr['Product_Description'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Web Designer\\x89Ûªs Guide to iOS (and Android) Apps, today @mention 10 a.m! {link} #sxsw',\n",
       " 'RT @mention Line for iPad 2 is longer today than yesterday. #SXSW  // are you getting in line again today just for fun?',\n",
       " 'Crazy that Apple is opening a temporary store in Austin tomorrow to handle the rabid #sxsw eye pad too seekers.',\n",
       " 'The lesson from Google One Pass: In this digital environment, users want to purchase across every platform with one tool. #sxsw #elonsxsw',\n",
       " 'RT @mention At the panel: &quot;Your mom has an ipad, designing for boomers&quot; #sxsw',\n",
       " 'RT @mention I think my effing hubby is in line for an #iPad 2. Can someone point him towards the line-up for wife number #2. #sxswi #sxsw',\n",
       " '&quot;Android users are not iPhone users. (They use the Options menu, and Contextual menu)&quot; ~@mention #sxsw',\n",
       " 'Wow! RT@mention We interrupt your regularly scheduled #sxsw geek programming with big news {link}  #google #circles',\n",
       " 'Google to Launch New Social Network Called Circles, Possibly Today {link} {link} #SXSW #in',\n",
       " '@mention  Welcome! Enjoy #Sxsw and ride anywhere in Austin for $10 . dwnld the #GroundLink app{link} booth 437',\n",
       " 'RT @mention Apple plans to Keep Austin Wired, opening a pop-up Apple store just for #SXSW {link}',\n",
       " 'Yeay! RT @mention New #UberSocial for #iPhone now in the App Store includes UberGuide to #SXSW sponsored by #Masha {link}',\n",
       " 'You know it is #SXSW season when there are 10 app updates whenever you open up the iPhone app store',\n",
       " 'RT @mention RT @mention Arriving in the US for #sxsw, will be looking for a MicroSIM for data in my iPhone // recommendations @mention',\n",
       " \"\\x89ÛÏ@mention It's not a rumor: Apple is opening up a temporary store in downtown Austin for #SXSW and the iPad 2 launch {link}\",\n",
       " 'Horrible repressed memories of the Apple spinning beach ball coming back at the #progressbar talk. #sxsw']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_prep = train_descr['Product_Description'].tolist()\n",
    "corpus_prep[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6352"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = set(corpus_prep)\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing corpus...\n",
      "Tokenizing...\n",
      "Number of tokens: 155260\n",
      "Building vocabulary...\n",
      "Length of vocabulary before pruning: 9009\n",
      "Length of vocabulary after pruning: 9009\n",
      "Index-encoding...\n",
      "Number of indices: 155260\n",
      "Saving file...\n"
     ]
    }
   ],
   "source": [
    "def preprocess_corpus(corpus_prep):\n",
    "    \"\"\"\n",
    "    Preprocesses the corpus\n",
    "    \n",
    "    Args: \n",
    "    corpus_prep - the text to be pre-processed\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(preprocessed_corpus_path):\n",
    "        print(\"Preprocessing corpus...\")\n",
    "\n",
    "        # Getting the vocabulary\n",
    "        #1. Tokenizing\n",
    "        print(\"Tokenizing...\")\n",
    "        corpus_string = [word_tokenize(description) for description in corpus_prep]\n",
    "        corpus_tokens = [item.lower() for sublist in corpus_string for item in sublist]\n",
    "        print(\"Number of tokens:\", len(corpus_tokens))\n",
    "        print(\"Building vocabulary...\")\n",
    "        word_counter = Counter()\n",
    "        word_counter.update(corpus_tokens)\n",
    "        print(\"Length of vocabulary before pruning:\", len(word_counter))\n",
    "        \n",
    "        #2. Derive the vocabulary - 10.000 most used words\n",
    "        vocabulary = [key for key, value in word_counter.most_common(most_common_words_number)]\n",
    "        print(\"Length of vocabulary after pruning:\", len(vocabulary))\n",
    "\n",
    "        #3. Converting to indices\n",
    "        print(\"Index-encoding...\")\n",
    "        indices = encode_sequence(corpus_tokens, vocabulary)\n",
    "        print(\"Number of indices:\", len(indices))\n",
    "\n",
    "        #4. Saving\n",
    "        print(\"Saving file...\")\n",
    "        pickle.dump((indices, vocabulary), open(preprocessed_corpus_path, \"wb\"))\n",
    "    else:\n",
    "        print(\"Corpus already preprocessed.\")\n",
    "\n",
    "preprocess_corpus(corpus_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue work on 15.11.2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Continue with manual tokenization & encoding \n",
    "1. Clean more the corpus see repo: https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/NLP%20Strategy%20I%20-%20Processing%20and%20Understanding%20Text.ipynb\n",
    "\n",
    "2. after creating the vocabulary, encode each description via encoder function in helpers (apply directly to df column?)\n",
    "\n",
    "3. create a dataformat as in the IMDB dataset - CONVERT TO TF DATASETS in https://stackoverflow.com/questions/58362316/how-do-i-go-from-pandas-dataframe-to-tensorflow-batchdataset-for-nlp (point III) \n",
    "\n",
    "II. Repeat it with Keras Tokenizer\n",
    "https://stackoverflow.com/questions/58362316/how-do-i-go-from-pandas-dataframe-to-tensorflow-batchdataset-for-nlp\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "\n",
    "III. Create a data format as in IMDB Dataset --> what is imdb_train in NLP sentiment analysis Tristan's Notebook\n",
    "https://stackoverflow.com/questions/58362316/how-do-i-go-from-pandas-dataframe-to-tensorflow-batchdataset-for-nlp\n",
    "\n",
    "IV. !!!!! Validation Set !!!!!! + think about k-fold cross validation as it is a small data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-sent",
   "language": "python",
   "name": "nlp-sent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
