{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/iskriyanavasileva/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /Users/iskriyanavasileva/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import progressbar\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import utils\n",
    "from keras.utils import to_categorical\n",
    "from nltk import word_tokenize\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('perluniprops')\n",
    "\n",
    "# This tokenizer is nice, but could cause problems.\n",
    "try:\n",
    "    from nltk.tokenize.moses import MosesDetokenizer\n",
    "    detokenizer = MosesDetokenizer()\n",
    "    use_moses_detokenizer = True\n",
    "except:\n",
    "    use_moses_detokenizer = False\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus parameters\n",
    "# corpus_path =\n",
    "\n",
    "# Preprocessing parameters\n",
    "preprocessed_corpus_path = \"product_descr_preprocessed.p\"\n",
    "most_common_words_number = 10000\n",
    "\n",
    "# Training parameters\n",
    "train_anyway = False\n",
    "model_path = \"product_descr.h5\"\n",
    "#dataset_size = 5000\n",
    "sequence_length = 30\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "hidden_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequence(sequence, vocabulary):\n",
    "    \"\"\" Encodes a sequence of tokens into a sequence of indices. \"\"\"\n",
    "\n",
    "    return [vocabulary.index(element) for element in sequence if element in vocabulary]\n",
    "\n",
    "\n",
    "def decode_indices(indices, vocabulary):\n",
    "    \"\"\" Decodes a sequence of indices and returns a string. \"\"\"\n",
    "\n",
    "    decoded_tokens = [vocabulary[index] for index in indices]\n",
    "    if use_moses_detokenizer == True:\n",
    "        return detokenizer.detokenize(decoded_tokens, return_str=True)\n",
    "    else:\n",
    "        return \" \".join(decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.getenv(\"HOME\")\n",
    "nlp_repo = os.path.join(home, 'git/nlp-product-sentiment-classification')\n",
    "src_dir = os.path.join(os.getcwd(), '..', 'src')\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = os.path.join(nlp_repo, 'data/03_processed/Train.csv')\n",
    "train_descr = pd.read_csv(train_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The Web Designer\\x89Ûªs Guide to iOS (and Android) Apps, today @mention 10 a.m! {link} #sxsw',\n",
       "       'RT @mention Line for iPad 2 is longer today than yesterday. #SXSW  // are you getting in line again today just for fun?',\n",
       "       'Crazy that Apple is opening a temporary store in Austin tomorrow to handle the rabid #sxsw eye pad too seekers.',\n",
       "       ...,\n",
       "       'RT @mention RT @mention Download 20+ free tracks from @mention Music Sampler @mention including @glove! {link} #SXSW',\n",
       "       \"OH at Texas Social Media Awards: 'You don't need to ask your mother anymore. Just Google it.' #sxswi #sxsw\",\n",
       "       '#Google launching a &quot;major&quot; new social network at #sxsw ... Wonder what that can be...'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_descr['Product_Description'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Web Designer\\x89Ûªs Guide to iOS (and Android) Apps, today @mention 10 a.m! {link} #sxsw',\n",
       " 'RT @mention Line for iPad 2 is longer today than yesterday. #SXSW  // are you getting in line again today just for fun?',\n",
       " 'Crazy that Apple is opening a temporary store in Austin tomorrow to handle the rabid #sxsw eye pad too seekers.',\n",
       " 'The lesson from Google One Pass: In this digital environment, users want to purchase across every platform with one tool. #sxsw #elonsxsw',\n",
       " 'RT @mention At the panel: &quot;Your mom has an ipad, designing for boomers&quot; #sxsw',\n",
       " 'RT @mention I think my effing hubby is in line for an #iPad 2. Can someone point him towards the line-up for wife number #2. #sxswi #sxsw',\n",
       " '&quot;Android users are not iPhone users. (They use the Options menu, and Contextual menu)&quot; ~@mention #sxsw',\n",
       " 'Wow! RT@mention We interrupt your regularly scheduled #sxsw geek programming with big news {link}  #google #circles',\n",
       " 'Google to Launch New Social Network Called Circles, Possibly Today {link} {link} #SXSW #in',\n",
       " '@mention  Welcome! Enjoy #Sxsw and ride anywhere in Austin for $10 . dwnld the #GroundLink app{link} booth 437',\n",
       " 'RT @mention Apple plans to Keep Austin Wired, opening a pop-up Apple store just for #SXSW {link}',\n",
       " 'Yeay! RT @mention New #UberSocial for #iPhone now in the App Store includes UberGuide to #SXSW sponsored by #Masha {link}',\n",
       " 'You know it is #SXSW season when there are 10 app updates whenever you open up the iPhone app store',\n",
       " 'RT @mention RT @mention Arriving in the US for #sxsw, will be looking for a MicroSIM for data in my iPhone // recommendations @mention',\n",
       " \"\\x89ÛÏ@mention It's not a rumor: Apple is opening up a temporary store in downtown Austin for #SXSW and the iPad 2 launch {link}\",\n",
       " 'Horrible repressed memories of the Apple spinning beach ball coming back at the #progressbar talk. #sxsw']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_prep = train_descr['Product_Description'].tolist()\n",
    "corpus_prep[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: \n",
    "* text wrangling: https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72 & https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/NLP%20Strategy%20I%20-%20Processing%20and%20Understanding%20Text.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6352"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = set(corpus_prep)\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Accented text'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD',\n",
    "                                 text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "remove_accented_chars('Sómě Áccěntěd těxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expand Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you all can not expand contractions I would think\n"
     ]
    }
   ],
   "source": [
    "print(contractions.fix(\"Y'all can't expand contractions I'd think\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun What do you think '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "remove_special_characters(\"Well this was fun! What do you think? 123#@!\",\n",
    "                          remove_digits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crash ! his crash yesterday , ours crash daily'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ !=\n",
    "                     '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "lemmatize_text(\n",
    "    \"My system keeps crashing! his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crash hi crash yesterday, our crash daili'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "simple_stemmer(\n",
    "    \"My system keeps crashing his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "# remove the negative words from stopword_list, as they are useful for a sentiment analysis\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', , stopwords , computer not'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [\n",
    "            token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [\n",
    "            token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "remove_stopwords(\"The, and, if are stopwords, computer is not\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Text Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus,\n",
    "                     contraction_expansion=True,\n",
    "                     accented_char_removal=True,\n",
    "                     text_lower_case=True,\n",
    "                     text_lemmatization=True,\n",
    "                     special_char_removal=True,\n",
    "                     stopword_removal=True,\n",
    "                     remove_digits=True\n",
    "                     ):\n",
    "    \"\"\"\n",
    "    This function normalizes the text and prepares it for the corpus pre-processing\n",
    "\n",
    "    Args: \n",
    "    - corpus - the text to be normalised\n",
    "    If set to true, the following functions are applied accordingly:\n",
    "    - contraction_expansion - expands phrases like I'm to I am\n",
    "    - accented_char_removal - sets characters like â to a\n",
    "    - text_lower_case - turns all characters to lower case\n",
    "    - text_lemmatization - lemmatizes text\n",
    "    - special_char_removal - removes characters like \"#\"\n",
    "    - stopword_removal - removes stop words such as \"the\", \"and\" etc.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    - normalized_corpus - the normalized text\n",
    "    \"\"\"\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus: \n",
    "        \n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        \n",
    "        # expand characters\n",
    "        if contraction_expansion:\n",
    "            doc = contractions.fix(doc)\n",
    "            \n",
    "        # lowercase the text\n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        \n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', doc)\n",
    "        \n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        \n",
    "        # remove special characters and / or digits\n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them\n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\"\\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)\n",
    "            \n",
    "            # remove extra whitespace\n",
    "            doc = re.sub(' +', ' ', doc)\n",
    "            \n",
    "            # remove stopwords\n",
    "            if stopword_removal:\n",
    "                doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "            normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Product_Description': 'RT @mention Line for iPad 2 is longer today than yesterday. #SXSW  // are you getting in line again today just for fun?',\n",
       " 'Clean_Text': 'rt mention line for ipad be long today than yesterday sxsw be you get in line again today just for fun '}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_descr['Clean_Text'] = normalize_corpus(train_descr['Product_Description'])\n",
    "norm_corpus = list(train_descr['Clean_Text'])\n",
    "train_descr.iloc[1][['Product_Description', 'Clean_Text']].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing of the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: \n",
    "* pre-procesing: DSR bootcamp\n",
    "* TF dataset: https://stackoverflow.com/questions/58362316/how-do-i-go-from-pandas-dataframe-to-tensorflow-batchdataset-for-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus_prep):\n",
    "    \"\"\"\n",
    "    Preprocesses the corpus\n",
    "\n",
    "    Args: \n",
    "    corpus_prep - the text to be pre-processed\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(preprocessed_corpus_path):\n",
    "        print(\"Preprocessing corpus...\")\n",
    "\n",
    "        # Getting the vocabulary\n",
    "        # 1. Tokenizing\n",
    "        print(\"Tokenizing...\")\n",
    "        corpus_string = [word_tokenize(description)\n",
    "                         for description in corpus_prep]\n",
    "        corpus_tokens = [item.lower()\n",
    "                         for sublist in corpus_string for item in sublist]\n",
    "        print(\"Number of tokens:\", len(corpus_tokens))\n",
    "        print(\"Building vocabulary...\")\n",
    "        word_counter = Counter()\n",
    "        word_counter.update(corpus_tokens)\n",
    "        print(\"Length of vocabulary before pruning:\", len(word_counter))\n",
    "\n",
    "        # 2. Derive the vocabulary - 10.000 most used words\n",
    "        vocabulary = [key for key, value in word_counter.most_common(\n",
    "            most_common_words_number)]\n",
    "        print(\"Length of vocabulary after pruning:\", len(vocabulary))\n",
    "\n",
    "        # 3. Converting to indices\n",
    "        print(\"Index-encoding...\")\n",
    "        indices = [encode_sequence(sequence, vocabulary)\n",
    "                   for sequence in corpus_string]\n",
    "        print(\"Number of indices:\", len(indices))\n",
    "\n",
    "        # 4. Saving\n",
    "        print(\"Saving file...\")\n",
    "        pickle.dump((indices, vocabulary), open(\n",
    "            preprocessed_corpus_path, \"wb\"))\n",
    "\n",
    "    else:\n",
    "        print(\"Corpus already preprocessed.\")\n",
    "\n",
    "\n",
    "preprocess_corpus(corpus_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, vocabulary = pd.read_pickle(r'product_descr_preprocessed.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning the Encoded Sequences into a Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    indices, padding='post')\n",
    "train_labels = train_descr['Sentiment'][:3].to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_seqs)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_seqs, train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check, if transformation went ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_descr, test_label = list(train_ds.take(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_descr.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_test_descr = decode_indices(test_descr, vocabulary)\n",
    "decoded_test_descr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-sent",
   "language": "python",
   "name": "nlp-sent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
