{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/iskriyanavasileva/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /Users/iskriyanavasileva/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import progressbar\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import sys\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('perluniprops')\n",
    "\n",
    "# This tokenizer is nice, but could cause problems.\n",
    "try:\n",
    "    from nltk.tokenize.moses import MosesDetokenizer\n",
    "    detokenizer = MosesDetokenizer()\n",
    "    use_moses_detokenizer = True\n",
    "except:\n",
    "    use_moses_detokenizer = False\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folders\n",
    "home = os.getenv(\"HOME\")\n",
    "nlp_repo = os.path.join(home, 'git/nlp-product-sentiment-classification')\n",
    "\n",
    "# Corpus parameters\n",
    "# corpus_path\n",
    "\n",
    "# Preprocessing parameters\n",
    "preprocessed_corpus_path = os.path.join(\n",
    "    nlp_repo, 'data/03_processed/product_descr_preprocessed.p')\n",
    "indices_test_path = os.path.join(\n",
    "    nlp_repo, 'data/03_processed/indices_test_path.p')\n",
    "# as our data is scarce, we will not need this parameter.\n",
    "# let's leave it in the code for parametization sake\n",
    "most_common_words_number = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: \n",
    "* text wrangling: https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72 & https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/NLP%20Strategy%20I%20-%20Processing%20and%20Understanding%20Text.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Accented text'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_accented_chars(text):\n",
    "    \"\"\" Sets characters like â to a \"\"\"\n",
    "\n",
    "    text = unicodedata.normalize('NFKD',\n",
    "                                 text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "remove_accented_chars('Sómě Áccěntěd těxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expand Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you all can not expand contractions I would think\n"
     ]
    }
   ],
   "source": [
    "print(contractions.fix(\"Y'all can't expand contractions I'd think\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crash ! his crash yesterday , ours crash daily'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    \"\"\" Returns the root of a word, for ex. went go \"\"\"\n",
    "\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ !=\n",
    "                     '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "lemmatize_text(\n",
    "    \"My system keeps crashing! his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crash hi crash yesterday, our crash daili'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_stemmer(text):\n",
    "    \"\"\" Returns the stem of a word, for ex. going go. It does not always overlap with the root \"\"\"\n",
    "\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "simple_stemmer(\n",
    "    \"My system keeps crashing his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun What do you think '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    \"\"\" Removes characters like \"#\" \"\"\"\n",
    "\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "remove_special_characters(\"Well this was fun! What do you think? 123#@!\",\n",
    "                          remove_digits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "# remove the negative words from stopword_list, as they are useful for a sentiment analysis\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', , stopwords , computer not'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    \"\"\" Removes words like \"the\", \"and\" etc. \"\"\"\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [\n",
    "            token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [\n",
    "            token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "remove_stopwords(\"The, and, if are stopwords, computer is not\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequence(sequence, vocabulary):\n",
    "    \"\"\" Encodes a sequence of tokens into a sequence of indices. \"\"\"\n",
    "    return [vocabulary.index(element) for element in sequence if element in vocabulary]\n",
    "\n",
    "\n",
    "def decode_indices(indices, vocabulary):\n",
    "    \"\"\" Decodes a sequence of indices and returns a string. \"\"\"\n",
    "    decoded_tokens = [vocabulary[index] for index in indices]\n",
    "    if use_moses_detokenizer == True:\n",
    "        return detokenizer.detokenize(decoded_tokens, return_str=True)\n",
    "    else:\n",
    "        return \" \".join(decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.getenv(\"HOME\")\n",
    "nlp_repo = os.path.join(home, 'git/nlp-product-sentiment-classification')\n",
    "src_dir = os.path.join(os.getcwd(), '..', 'src')\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = os.path.join(nlp_repo, 'data/03_processed/Train.csv')\n",
    "train_descr = pd.read_csv(train_csv_path)\n",
    "\n",
    "test_csv_path = os.path.join(nlp_repo, 'data/03_processed/Test.csv')\n",
    "test_descr = pd.read_csv(test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The Web Designer\\x89Ûªs Guide to iOS (and Android) Apps, today @mention 10 a.m! {link} #sxsw',\n",
       "       'RT @mention Line for iPad 2 is longer today than yesterday. #SXSW  // are you getting in line again today just for fun?',\n",
       "       'Crazy that Apple is opening a temporary store in Austin tomorrow to handle the rabid #sxsw eye pad too seekers.',\n",
       "       ...,\n",
       "       'RT @mention RT @mention Download 20+ free tracks from @mention Music Sampler @mention including @glove! {link} #SXSW',\n",
       "       \"OH at Texas Social Media Awards: 'You don't need to ask your mother anymore. Just Google it.' #sxswi #sxsw\",\n",
       "       '#Google launching a &quot;major&quot; new social network at #sxsw ... Wonder what that can be...'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_descr['Product_Description'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Web Designer\\x89Ûªs Guide to iOS (and Android) Apps, today @mention 10 a.m! {link} #sxsw',\n",
       " 'RT @mention Line for iPad 2 is longer today than yesterday. #SXSW  // are you getting in line again today just for fun?',\n",
       " 'Crazy that Apple is opening a temporary store in Austin tomorrow to handle the rabid #sxsw eye pad too seekers.',\n",
       " 'The lesson from Google One Pass: In this digital environment, users want to purchase across every platform with one tool. #sxsw #elonsxsw',\n",
       " 'RT @mention At the panel: &quot;Your mom has an ipad, designing for boomers&quot; #sxsw',\n",
       " 'RT @mention I think my effing hubby is in line for an #iPad 2. Can someone point him towards the line-up for wife number #2. #sxswi #sxsw',\n",
       " '&quot;Android users are not iPhone users. (They use the Options menu, and Contextual menu)&quot; ~@mention #sxsw',\n",
       " 'Wow! RT@mention We interrupt your regularly scheduled #sxsw geek programming with big news {link}  #google #circles',\n",
       " 'Google to Launch New Social Network Called Circles, Possibly Today {link} {link} #SXSW #in',\n",
       " '@mention  Welcome! Enjoy #Sxsw and ride anywhere in Austin for $10 . dwnld the #GroundLink app{link} booth 437',\n",
       " 'RT @mention Apple plans to Keep Austin Wired, opening a pop-up Apple store just for #SXSW {link}',\n",
       " 'Yeay! RT @mention New #UberSocial for #iPhone now in the App Store includes UberGuide to #SXSW sponsored by #Masha {link}',\n",
       " 'You know it is #SXSW season when there are 10 app updates whenever you open up the iPhone app store',\n",
       " 'RT @mention RT @mention Arriving in the US for #sxsw, will be looking for a MicroSIM for data in my iPhone // recommendations @mention',\n",
       " \"\\x89ÛÏ@mention It's not a rumor: Apple is opening up a temporary store in downtown Austin for #SXSW and the iPad 2 launch {link}\",\n",
       " 'Horrible repressed memories of the Apple spinning beach ball coming back at the #progressbar talk. #sxsw']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_prep = train_descr['Product_Description'].tolist()\n",
    "corpus_prep[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Text Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus,\n",
    "                     accented_char_removal=True,\n",
    "                     contraction_expansion=True,\n",
    "\n",
    "                     text_lower_case=True,\n",
    "                     text_lemmatization=True,\n",
    "                     special_char_removal=True,\n",
    "                     stopword_removal=True,\n",
    "                     remove_digits=True\n",
    "                     ):\n",
    "    \"\"\"\n",
    "    This function normalizes the text and prepares it for the corpus pre-processing\n",
    "\n",
    "    Args: \n",
    "    - corpus - the text to be normalised\n",
    "    If set to true, the following functions are applied accordingly:\n",
    "    - accented_char_removal - sets characters like â to a\n",
    "    - contraction_expansion - expands phrases like I'm to I am\n",
    "    - text_lower_case - turns all characters to lower case\n",
    "    - text_lemmatization - lemmatizes text\n",
    "    - special_char_removal - removes characters like \"#\"\n",
    "    - stopword_removal - removes stop words such as \"the\", \"and\" etc.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    - normalized_corpus - the normalized text\n",
    "    \"\"\"\n",
    "\n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "\n",
    "        # expand characters\n",
    "        if contraction_expansion:\n",
    "            doc = contractions.fix(doc)\n",
    "\n",
    "        # lowercase the text\n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', doc)\n",
    "\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "\n",
    "        # remove special characters and / or digits\n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them\n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\"\\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)\n",
    "\n",
    "            # remove extra whitespace\n",
    "            doc = re.sub(' +', ' ', doc)\n",
    "\n",
    "            # remove stopwords\n",
    "            if stopword_removal:\n",
    "                doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "\n",
    "            normalized_corpus.append(doc)\n",
    "\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_descr['Clean_Text'] = normalize_corpus(\n",
    "#    train_descr['Product_Description'])\n",
    "#corpus_norm = list(train_descr['Clean_Text'])\n",
    "#corpus_norm[:3]\n",
    "\n",
    "#train_descr.iloc[1][['Product_Description', 'Clean_Text']].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing of the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: \n",
    "* pre-procesing: DSR bootcamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing corpus...\n",
      "Normalising & Tokenizing...\n",
      "Number of tokens: 77698\n",
      "Building vocabulary...\n",
      "Length of vocabulary before pruning: 7119\n",
      "Length of vocabulary after pruning: 7119\n",
      "Index-encoding...\n",
      "Number of indices: 6364\n",
      "Saving file...\n",
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "def preprocess_corpus(corpus_prep):\n",
    "    \"\"\"\n",
    "    Preprocesses the corpus\n",
    "\n",
    "    Args: \n",
    "    corpus_prep - the text to be pre-processed\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(preprocessed_corpus_path):\n",
    "        print(\"Preprocessing corpus...\")\n",
    "\n",
    "        # Getting the vocabulary\n",
    "        # 1. Normalizing & Tokenizing\n",
    "        print(\"Normalising & Tokenizing...\")\n",
    "        corpus_norm = normalize_corpus(corpus_prep)\n",
    "        corpus_string = [word_tokenize(description) for description in corpus_norm]\n",
    "        corpus_tokens = [item for sublist in corpus_string for item in sublist]\n",
    "\n",
    "        print(\"Number of tokens:\", len(corpus_tokens))\n",
    "        print(\"Building vocabulary...\")\n",
    "        word_counter = Counter()\n",
    "        word_counter.update(corpus_tokens)\n",
    "        print(\"Length of vocabulary before pruning:\", len(word_counter))\n",
    "\n",
    "        # 2. Derive the vocabulary - 10.000 most used words\n",
    "        vocabulary = [key for key, value in word_counter.most_common(\n",
    "            most_common_words_number)]\n",
    "        print(\"Length of vocabulary after pruning:\", len(vocabulary))\n",
    "\n",
    "        # 3. Converting to indices\n",
    "        print(\"Index-encoding...\")\n",
    "        indices = [encode_sequence(sequence, vocabulary)\n",
    "                   for sequence in corpus_string]\n",
    "        print(\"Number of indices:\", len(indices))\n",
    "\n",
    "        # 4. Saving\n",
    "        print(\"Saving file...\")\n",
    "        pickle.dump((indices, vocabulary), open(\n",
    "            preprocessed_corpus_path, \"wb\"))\n",
    "        print(\"File Saved\")\n",
    "\n",
    "    else:\n",
    "        print(\"Corpus already preprocessed.\")\n",
    "\n",
    "\n",
    "preprocess_corpus(train_descr['Product_Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_test(corpus_prep_test):\n",
    "    \"\"\" Nomralizes and Tokenizes the test data, i.e. uses  \"\"\"\n",
    "    corpus_norm_test = normalize_corpus(corpus_prep_test)\n",
    "    \n",
    "    corpus_string_test = [word_tokenize(description) for description in corpus_norm_test]\n",
    "    \n",
    "    _, vocabulary = pd.read_pickle(preprocessed_corpus_path)\n",
    "    \n",
    "    indices_test = [encode_sequence(sequence, vocabulary)\n",
    "                   for sequence in corpus_string_test]\n",
    "    \n",
    "    pickle.dump((indices_test, vocabulary), open(\n",
    "            indices_test_path, \"wb\"))\n",
    "\n",
    "indices_test(test_descr['Product_Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, vocabulary = pd.read_pickle(preprocessed_corpus_path)\n",
    "indices_test, _ = pd.read_pickle(indices_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 1, 21, 0, 9, 7, 294, 12, 1, 28, 399, 535, 1162, 28, 552],\n",
       " [4, 1, 7, 581, 11, 724, 94, 1777, 0],\n",
       " [4, 1, 73, 1, 166, 307, 322, 759, 6, 8, 166, 63, 234, 1065, 109, 0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[154, 3719, 294, 450, 17, 11, 18, 1, 2, 0],\n",
       " [4, 1, 24, 3, 137, 18, 513, 0, 15, 24, 18, 145],\n",
       " [451, 6, 22, 44, 8, 12, 161, 710, 3720, 0, 1354, 3721, 3722]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['web designeruas guide ios android app today mention link sxsw',\n",
       " 'rt mention line ipad long today yesterday sxsw get line today fun',\n",
       " 'crazy apple open temporary store austin tomorrow handle rabid sxsw eye pad seeker']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_indices = [decode_indices(index, vocabulary) for index in indices[:3]]\n",
    "decoded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_prep_test = train_descr['Product_Description'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_norm_test = normalize_corpus(corpus_prep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['web',\n",
       "  'designeruas',\n",
       "  'guide',\n",
       "  'ios',\n",
       "  'android',\n",
       "  'app',\n",
       "  'today',\n",
       "  'mention',\n",
       "  'link',\n",
       "  'sxsw'],\n",
       " ['rt',\n",
       "  'mention',\n",
       "  'line',\n",
       "  'ipad',\n",
       "  'long',\n",
       "  'today',\n",
       "  'yesterday',\n",
       "  'sxsw',\n",
       "  'get',\n",
       "  'line',\n",
       "  'today',\n",
       "  'fun'],\n",
       " ['crazy',\n",
       "  'apple',\n",
       "  'open',\n",
       "  'temporary',\n",
       "  'store',\n",
       "  'austin',\n",
       "  'tomorrow',\n",
       "  'handle',\n",
       "  'rabid',\n",
       "  'sxsw',\n",
       "  'eye',\n",
       "  'pad',\n",
       "  'seeker']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_string_test = [word_tokenize(description) for description in corpus_norm_test]\n",
    "corpus_string_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['web',\n",
       " 'designeruas',\n",
       " 'guide',\n",
       " 'ios',\n",
       " 'android',\n",
       " 'app',\n",
       " 'today',\n",
       " 'mention',\n",
       " 'link',\n",
       " 'sxsw',\n",
       " 'rt',\n",
       " 'mention',\n",
       " 'line',\n",
       " 'ipad',\n",
       " 'long',\n",
       " 'today',\n",
       " 'yesterday',\n",
       " 'sxsw',\n",
       " 'get',\n",
       " 'line',\n",
       " 'today',\n",
       " 'fun',\n",
       " 'crazy',\n",
       " 'apple',\n",
       " 'open',\n",
       " 'temporary',\n",
       " 'store',\n",
       " 'austin',\n",
       " 'tomorrow',\n",
       " 'handle',\n",
       " 'rabid',\n",
       " 'sxsw',\n",
       " 'eye',\n",
       " 'pad',\n",
       " 'seeker']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokens_test = [item for sublist in corpus_string_test for item in sublist]\n",
    "corpus_tokens_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'web': 1,\n",
       "         'designeruas': 1,\n",
       "         'guide': 1,\n",
       "         'ios': 1,\n",
       "         'android': 1,\n",
       "         'app': 1,\n",
       "         'today': 3,\n",
       "         'mention': 2,\n",
       "         'link': 1,\n",
       "         'sxsw': 3,\n",
       "         'rt': 1,\n",
       "         'line': 2,\n",
       "         'ipad': 1,\n",
       "         'long': 1,\n",
       "         'yesterday': 1,\n",
       "         'get': 1,\n",
       "         'fun': 1,\n",
       "         'crazy': 1,\n",
       "         'apple': 1,\n",
       "         'open': 1,\n",
       "         'temporary': 1,\n",
       "         'store': 1,\n",
       "         'austin': 1,\n",
       "         'tomorrow': 1,\n",
       "         'handle': 1,\n",
       "         'rabid': 1,\n",
       "         'eye': 1,\n",
       "         'pad': 1,\n",
       "         'seeker': 1})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter_test = Counter()\n",
    "word_counter_test.update(corpus_tokens_test)\n",
    "word_counter_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today',\n",
       " 'sxsw',\n",
       " 'mention',\n",
       " 'line',\n",
       " 'web',\n",
       " 'designeruas',\n",
       " 'guide',\n",
       " 'ios',\n",
       " 'android',\n",
       " 'app',\n",
       " 'link',\n",
       " 'rt',\n",
       " 'ipad',\n",
       " 'long',\n",
       " 'yesterday',\n",
       " 'get',\n",
       " 'fun',\n",
       " 'crazy',\n",
       " 'apple',\n",
       " 'open',\n",
       " 'temporary',\n",
       " 'store',\n",
       " 'austin',\n",
       " 'tomorrow',\n",
       " 'handle',\n",
       " 'rabid',\n",
       " 'eye',\n",
       " 'pad',\n",
       " 'seeker']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_test = [key for key, value in word_counter_test.most_common(\n",
    "            most_common_words_number)]\n",
    "vocabulary_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today',\n",
       " 'sxsw',\n",
       " 'mention',\n",
       " 'line',\n",
       " 'web',\n",
       " 'designeruas',\n",
       " 'guide',\n",
       " 'ios',\n",
       " 'android',\n",
       " 'app',\n",
       " 'link',\n",
       " 'rt',\n",
       " 'ipad',\n",
       " 'long',\n",
       " 'yesterday',\n",
       " 'get',\n",
       " 'fun',\n",
       " 'crazy',\n",
       " 'apple',\n",
       " 'open',\n",
       " 'temporary',\n",
       " 'store',\n",
       " 'austin',\n",
       " 'tomorrow',\n",
       " 'handle',\n",
       " 'rabid',\n",
       " 'eye',\n",
       " 'pad',\n",
       " 'seeker']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocabulary_new = vocabulary_test[:]\n",
    "#vocabulary_test[0:0] = ['abcd']\n",
    "vocabulary_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 5, 6, 7, 8, 9, 0, 2, 10, 1],\n",
       " [11, 2, 3, 12, 13, 0, 14, 1, 15, 3, 0, 16],\n",
       " [17, 18, 19, 20, 21, 22, 23, 24, 25, 1, 26, 27, 28]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_test = [encode_sequence(sequence, vocabulary_test) for sequence in corpus_string_test]\n",
    "indices_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "web\n",
      "designeruas\n",
      "guide\n",
      "ios\n",
      "android\n",
      "app\n",
      "today\n",
      "mention\n",
      "link\n",
      "sxsw\n",
      "rt\n",
      "mention\n",
      "line\n",
      "ipad\n",
      "long\n",
      "today\n",
      "yesterday\n",
      "sxsw\n",
      "get\n",
      "line\n",
      "today\n",
      "fun\n",
      "crazy\n",
      "apple\n",
      "open\n",
      "temporary\n",
      "store\n",
      "austin\n",
      "tomorrow\n",
      "handle\n",
      "rabid\n",
      "sxsw\n",
      "eye\n",
      "pad\n",
      "seeker\n"
     ]
    }
   ],
   "source": [
    "#[item for sublist in corpus_string for item in sublist] vocabulary_test[index]\n",
    "decoded_tokens_test = [print(vocabulary_test[index]) for sublist in indices_test for index in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['web designeruas guide ios android app today mention link sxsw',\n",
       " 'rt mention line ipad long today yesterday sxsw get line today fun',\n",
       " 'crazy apple open temporary store austin tomorrow handle rabid sxsw eye pad seeker']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_test_descr = [decode_indices(indices, vocabulary_test) for indices in indices_test]\n",
    "decoded_test_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-sent",
   "language": "python",
   "name": "nlp-sent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
